{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle file dependencies\n",
    "from tacc_stats.pickler.job_stats import Job\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System dependencies\n",
    "from os import listdir\n",
    "import time as clock\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation dependencies\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of all pickled jobs via comet\n",
    "# new_dir = '/oasis/projects/nsf/sys200/stats/xsede_stats/archive'\n",
    "source_dir = '/oasis/projects/nsf/sys200/tcooper/xsede_stats/comet_pickles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom data cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCRIPTION:\n",
    "    # given a date directory, return all pickle files\n",
    "def drop_invalid( src_dir, date ):\n",
    "    job_pickles = []\n",
    "    \n",
    "    for jobid in listdir(src_dir+date): \n",
    "        try:\n",
    "            pickle_file = open( src_dir+date+'/'+jobid, 'rb')\n",
    "            job = pickle.load(pickle_file)\n",
    "            job_pickles.append(jobid)\n",
    "            pickle_file.close()\n",
    "        except:\n",
    "            next \n",
    "    return job_pickles\n",
    "            \n",
    "# DESCRIPTION:\n",
    "    # given a date directory of pickle files\n",
    "    # return files for jobs which ran at/above minimum\n",
    "        # Note: minimum = 4  >>>  1 hour run time for job\n",
    "def drop_below( src_dir, pickles, minimum=4 ):\n",
    "    job_pickles = []\n",
    "    \n",
    "    for jobid in listdir(src_dir+date): \n",
    "        try:\n",
    "            pickle_file = open( src_dir+date+'/'+jobid, 'rb')\n",
    "            job = pickle.load(pickle_file)\n",
    "            if (len(job.times) > minimum):\n",
    "                job_pickles.append(jobid)\n",
    "            pickle_file.close() \n",
    "        except:\n",
    "            next \n",
    "    return job_pickles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Info (Pre-cleaning)\n",
    "Collections of date directories in source_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates_list preserves order from parent dir\n",
    "dates_list = [ date for date in listdir(source_dir) if len(listdir(source_dir+date)) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys: dates, values: number of potential job files\n",
    "size_info = {date:len(listdir(source_dir+date)) for date in dates_list}\n",
    "\n",
    "# sorted size_info\n",
    "by_size = sorted(size_info, key=size_info.get, reverse=False)\n",
    "\n",
    "# total num of files to parse\n",
    "n = float(sum([value for key,value in size_info.iteritems()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of date folders to parse:\t97\n",
      "Number of total files to parse:\t\t2199159.0\n"
     ]
    }
   ],
   "source": [
    "print \"Number of date folders to parse:\\t\", len(dates_list)\n",
    "print \"Number of total files to parse:\\t\\t\", n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Info (Post-cleaning)\n",
    "Collections of date directories in source_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in size_info:\n",
    "    if date in listdir('./modules/data/dates(2016)/'):\n",
    "        next\n",
    "    else:\n",
    "        pickles = drop_invalid( source_dir, date )\n",
    "        valid = drop_below( source_dir, pickles )\n",
    "        with open('./modules/data/dates(2016)/'+date, 'wb') as fp:\n",
    "            for jobid in valid:\n",
    "                fp.write('%s\\n' % jobid)\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listdir('./modules/data/dates(2016)/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_files = {}\n",
    "\n",
    "for date in dates_list:\n",
    "    pickles = drop_invalid( source_dir, date )\n",
    "    acceptable = drop_below( source_dir, pickles )\n",
    "    if acceptable: target_files[date] = acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys: dates, values: number of potential job files\n",
    "size_info = {date:len(target_files[date]) for date in target_files.keys()}\n",
    "\n",
    "# sorted size_info\n",
    "by_size = sorted(size_info, key=size_info.get, reverse=True)\n",
    "\n",
    "# total num of files to parse\n",
    "n = float(sum([value for key,value in size_info.iteritems()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Number of date folders to parse:\\t\", len(target_files.keys())\n",
    "print \"Number of total files to parse:\\t\", n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
