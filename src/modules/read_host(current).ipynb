{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System dependencies\n",
    "from os import listdir\n",
    "import time as clock\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save to\n",
    "save_dir = '../data/continued/'\n",
    "\n",
    "# Directory of prev job scans\n",
    "id_dir = '../data/labels/IDs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of recent saved comet jobs\n",
    "source_dir = '/oasis/projects/nsf/sys200/stats/xsede_stats/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archive_of_archive',\n",
       " 'gordon_hostfile_logs',\n",
       " 'gordon_pickles',\n",
       " 'comet_accounting',\n",
       " 'gordon_accounting',\n",
       " 'comet_pickles',\n",
       " 'archive',\n",
       " '.htaccess',\n",
       " 'comet_hostfile_logs']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = listdir(source_dir)\n",
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/sys200/stats/xsede_stats/gordon_hostfile_logs\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/gordon_pickles\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/gordon_accounting\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/comet_pickles\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/.htaccess\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/comet_hostfile_logs\n"
     ]
    }
   ],
   "source": [
    "possible = [ source_dir+file_name for file_name in listdir(source_dir) ]\n",
    "\n",
    "for item in possible:\n",
    "    try:\n",
    "        listdir(item)\n",
    "    except:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = { 'aofa': source_dir+'archive_of_archive',\n",
    "         'job_info': source_dir+'comet_accounting',\n",
    "         'arc': source_dir+'archive'\n",
    "         #'host_info': source_dir+'comet_hostfile_logs',\n",
    "         #'old_pickles': source_dir+'comet_pickles'\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/sys200/stats/xsede_stats/archive_of_archive\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/comet_accounting\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/archive\n"
     ]
    }
   ],
   "source": [
    "for key,loc in locs.items():\n",
    "    print(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time( spec=None ):\n",
    "    try:\n",
    "        return clock.strftime(\"%Y/%m/%d %H:%M:%S\", clock.localtime( spec ))\n",
    "    except:\n",
    "        return \"0000/00/00 00:00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_header( line ):\n",
    "    if line.find(\" \") < 0:\n",
    "        try:\n",
    "            return line[0] == '%'\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "    else:\n",
    "        chunks = line.split(\" \")\n",
    "        try:\n",
    "            return (chunks[0][0] == '%') or ( chunks[2].find(\"comet\") >= 0 )\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_job( chunk ):\n",
    "    return chunk.find(\"-\") == -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_txt( txt_file ):\n",
    "    \n",
    "    with open( txt_file, \"rt\" ) as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_txt( gzipped ):\n",
    "    \n",
    "    with gzip.open( gzipped, 'rt') as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_save( obj, label=get_time() ):\n",
    "    \n",
    "    try:\n",
    "        out_file = open( label, 'wb')\n",
    "        pickle.dump( obj, out_file)\n",
    "        \n",
    "        # double check save\n",
    "        check_cpicore_set = pickle.load(open(cpiset_out, 'rb'))\n",
    "        check_cpicore_set = None\n",
    "        \n",
    "    except:\n",
    "        \"There was a problem pickling the object - Save manually.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_dict( rules, info ):\n",
    "    rules_list = rules.split(\"|\")\n",
    "    \n",
    "    if len(rules_list) != len(info):\n",
    "        return {}\n",
    "    \n",
    "    else:\n",
    "        return { rules_list[i]:info[i] for i in range(len(rules_list)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def host_to_info_dict( zip_txt ):\n",
    "    contents = unzip_txt( zip_txt )\n",
    "    out_dict = { \"Host\": {} }\n",
    "    host_info = { \"Timestamp\":{} }\n",
    "    info_dict = { \"Data\":{},\n",
    "                    \"Job\":\"N/A\",\n",
    "                    \"Schemas\":{},\n",
    "                    \"Specs\":[]\n",
    "                }\n",
    "    \n",
    "    for line in contents:\n",
    "            \n",
    "        if line[0] == \"$\":\n",
    "            info_dict[\"Specs\"].append( format_spec( line ) )\n",
    "            \n",
    "        elif line[0] == \"!\":\n",
    "            info_dict[\"Schemas\"].update( format_schema( line ) )\n",
    "        \n",
    "        else:\n",
    "            host_name = \"\"\n",
    "            timestamp = \"\"\n",
    "            \n",
    "            if len(line) < 3 or check_header( line ):\n",
    "                header_dict = format_header( line )\n",
    "                \n",
    "                if header_dict:\n",
    "                    if check_job( header_dict[\"Jobid\"] ):\n",
    "                        info_dict[\"Job\"] = { \"Jobid\": header_dict[\"Jobid\"] }\n",
    "                        \n",
    "                    host_name = header_dict[\"Host\"]\n",
    "                    timestamp = header_dict[\"Timestamp\"]\n",
    "                    \n",
    "                    time_data = { timestamp : {} }\n",
    "                    out_dict[host_name] = time_data\n",
    "                    \n",
    "            else:        \n",
    "                incoming = format_data( line )\n",
    "                found_stat = incoming[\"Stat\"]\n",
    "                info_dict[\"Data\"][ found_stat ] = incoming\n",
    "                \n",
    "    \n",
    "    \n",
    "                \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_header( line ):\n",
    "    chunks = line.split(\" \")\n",
    "    \n",
    "    try:\n",
    "        if chunks[0][0] == '%':\n",
    "            return {}\n",
    "        else:\n",
    "            return { \"Timestamp\": get_time( chunks[0] ), \n",
    "                     \"Jobid\": chunks[1],\n",
    "                     \"Host\": chunks[2][:11] }\n",
    "        \n",
    "    except:\n",
    "        print(line)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_spec( line ):\n",
    "    return line[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_schema( line ):\n",
    "    chunks = line.partition(\" \")\n",
    "    \n",
    "    stat = chunks[0][1:]\n",
    "    schemas = chunks[2:]\n",
    "    \n",
    "    return { stat:schemas }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data( line ):\n",
    "    chunks = line.split(\" \")\n",
    "    \n",
    "    stat = chunks[0]\n",
    "    dev = chunks[1]\n",
    "    data = chunks[2:-1]\n",
    "    \n",
    "    return { \"Stat\": stat, \"Device\": dev, \"Data\": data }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse file in archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_data = [ locs['arc']+'/'+stamp for stamp in listdir(locs['arc']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/oasis/projects/nsf/sys200/stats/xsede_stats/archive/comet-10-14.sdsc.edu/1587951665.gz'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = listdir(arc_data[0])[0]\n",
    "temp_loc = arc_data[0]+'/'+temp\n",
    "temp_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Host': {},\n",
       " '': {'Timestamp': {},\n",
       "  '': {'Data': {'Stat': 'vm',\n",
       "    'Device': '-',\n",
       "    'Data': ['3',\n",
       "     '5914304241',\n",
       "     '198784295',\n",
       "     '0',\n",
       "     '0',\n",
       "     '97101369100',\n",
       "     '98990571165',\n",
       "     '961473995',\n",
       "     '163956359',\n",
       "     '185544931657',\n",
       "     '4036335',\n",
       "     '156603337',\n",
       "     '0',\n",
       "     '26253111',\n",
       "     '15755',\n",
       "     '0',\n",
       "     '72880512',\n",
       "     '0',\n",
       "     '85196',\n",
       "     '286182',\n",
       "     '40',\n",
       "     '27390',\n",
       "     '325',\n",
       "     '2',\n",
       "     '1',\n",
       "     '0']},\n",
       "   'Job': {'Jobid': '32951517'},\n",
       "   'Schemas': {'block': ('rd_ios,E rd_merges,E rd_sectors,E,U=512B rd_ticks,E,U=ms wr_ios,E wr_merges,E wr_sectors,E,U=512B wr_ticks,E,U=ms in_flight io_ticks,E,U=ms time_in_queue,E,U=ms\\n',),\n",
       "    'cpu': ('user,E,U=cs nice,E,U=cs system,E,U=cs idle,E,U=cs iowait,E,U=cs irq,E,U=cs softirq,E,U=cs\\n',),\n",
       "    'ib': ('excessive_buffer_overrun_errors,E,W=32 link_downed,E,W=32 link_error_recovery,E,W=32 local_link_integrity_errors,E,W=32 port_rcv_constraint_errors,E,W=32 port_rcv_data,E,U=4B,W=32 port_rcv_errors,E,W=32 port_rcv_packets,E,W=32 port_rcv_remote_physical_errors,E,W=32 port_rcv_switch_relay_errors,E,W=32 port_xmit_constraint_errors,E,W=32 port_xmit_data,E,U=4B,W=32 port_xmit_discards,E,W=32 port_xmit_packets,E,W=32 port_xmit_wait,E,U=ms,W=32 symbol_error,E,W=32 VL15_dropped,E,W=32\\n',),\n",
       "    'ib_ext': ('port_select,C counter_select,C port_xmit_data,E,U=4B port_rcv_data,E,U=4B port_xmit_pkts,E port_rcv_pkts,E port_unicast_xmit_pkts,E port_unicast_rcv_pkts,E port_multicast_xmit_pkts,E port_multicast_rcv_pkts,E\\n',),\n",
       "    'ib_sw': ('rx_bytes,E,U=4B rx_packets,E tx_bytes,E,U=4B tx_packets,E\\n',),\n",
       "    'intel_8pmc3': ('CTL0,C CTL1,C CTL2,C CTL3,C CTL4,C CTL5,C CTL6,C CTL7,C CTR0,E,W=48 CTR1,E,W=48 CTR2,E,W=48 CTR3,E,W=48 CTR4,E,W=48 CTR5,E,W=48 CTR6,E,W=48 CTR7,E,W=48 FIXED_CTR0,E,W=48 FIXED_CTR1,E,W=48 FIXED_CTR2,E,W=48\\n',),\n",
       "    'intel_hsw_cbo': ('CTL0,C CTL1,C CTL2,C CTL3,C CTR0,E,W=48 CTR1,E,W=48 CTR2,E,W=48 CTR3,E,W=48\\n',),\n",
       "    'intel_hsw_hau': ('CTL0,C CTL1,C CTL2,C CTL3,C CTR0,E,W=48 CTR1,E,W=48 CTR2,E,W=48 CTR3,E,W=48\\n',),\n",
       "    'intel_hsw_imc': ('CTL0,C CTL1,C CTL2,C CTL3,C CTR0,E,W=48 CTR1,E,W=48 CTR2,E,W=48 CTR3,E,W=48 FIXED_CTR,E,W=48\\n',),\n",
       "    'intel_hsw_qpi': ('CTL0,C CTL1,C CTL2,C CTL3,C CTR0,E,U=flt,W=48 CTR1,E,U=flt,W=48 CTR2,E,U=flt,W=48 CTR3,E,U=flt,W=48\\n',),\n",
       "    'intel_hsw_r2pci': ('CTL0,C CTL1,C CTL2,C CTL3,C CTR0,E,W=44 CTR1,E,W=44 CTR2,E,W=44 CTR3,E,W=44\\n',),\n",
       "    'intel_rapl': ('MSR_PKG_ENERGY_STATUS,E,U=mJ,W=32 MSR_PP0_ENERGY_STATUS,E,U=mJ,W=32 MSR_DRAM_ENERGY_STATUS,E,U=mJ,W=32\\n',),\n",
       "    'llite': ('read_bytes,E,U=B write_bytes,E,U=B direct_read,E,U=B direct_write,E,U=B osc_read,E,U=B osc_write,E,U=B dirty_pages_hits,E dirty_pages_misses,E ioctl,E open,E close,E mmap,E seek,E fsync,E setattr,E truncate,E flock,E getattr,E statfs,E alloc_inode,E setxattr,E getxattr,E listxattr,E removexattr,E inode_permission,E readdir,E create,E lookup,E link,E unlink,E symlink,E mkdir,E rmdir,E mknod,E rename,E\\n',),\n",
       "    'mdc': ('ldlm_cancel,E mds_close,E mds_getattr,E mds_getattr_lock,E mds_getxattr,E mds_readpage,E mds_statfs,E mds_sync,E reqs,E wait,E,U=us\\n',),\n",
       "    'mem': ('MemTotal,U=KB MemFree,U=KB MemUsed,U=KB Active,U=KB Inactive,U=KB Dirty,U=KB Writeback,U=KB FilePages,U=KB Mapped,U=KB AnonPages,U=KB PageTables,U=KB NFS_Unstable,U=KB Bounce,U=KB Slab,U=KB AnonHugePages,U=KB HugePages_Total HugePages_Free\\n',),\n",
       "    'net': ('collisions,E multicast,E rx_bytes,E,U=B rx_compressed,E rx_crc_errors,E rx_dropped,E rx_errors,E rx_fifo_errors,E rx_frame_errors,E rx_length_errors,E rx_missed_errors,E rx_over_errors,E rx_packets,E tx_aborted_errors,E tx_bytes,E,U=B tx_carrier_errors,E tx_compressed,E tx_dropped,E tx_errors,E tx_fifo_errors,E tx_heartbeat_errors,E tx_packets,E tx_window_errors,E\\n',),\n",
       "    'numa': ('numa_hit,E numa_miss,E numa_foreign,E interleave_hit,E local_node,E other_node,E\\n',),\n",
       "    'osc': ('read_bytes,E,U=B write_bytes,E,U=B ost_destroy,E ost_punch,E ost_read,E ost_setattr,E ost_statfs,E ost_write,E reqs,E wait,E,U=us\\n',),\n",
       "    'proc': ('Uid VmPeak,U=kB VmSize,U=kB VmLck,U=kB VmHWM,U=kB VmRSS,U=kB VmData,U=kB VmStk,U=kB VmExe,U=kB VmLib,U=kB VmPTE,U=kB VmSwap,U=kB Threads\\n',),\n",
       "    'ps': ('ctxt,E processes,E load_1 load_5 load_15 nr_running nr_threads\\n',),\n",
       "    'sysv_shm': ('mem_used,U=B segs_used\\n',),\n",
       "    'tmpfs': ('bytes_used,U=B files_used\\n',),\n",
       "    'vfs': ('dentry_use file_use inode_use\\n',),\n",
       "    'vm': ('nr_anon_transparent_hugepages pgpgin,E,U=KB pgpgout,E,U=KB pswpin,E pswpout,E pgalloc_normal,E pgfree,E pgactivate,E pgdeactivate,E pgfault,E pgmajfault,E pgrefill_normal,E pgsteal_normal,E pgscan_kswapd_normal,E pgscan_direct_normal,E pginodesteal,E slabs_scanned,E kswapd_steal,E kswapd_inodesteal,E pageoutrun,E allocstall,E pgrotated,E thp_fault_alloc,E thp_fault_fallback,E thp_collapse_alloc,E thp_collapse_alloc_failed,E thp_split,E\\n',)},\n",
       "   'Specs': ['tacc_stats 2.3.4',\n",
       "    'hostname comet-10-14.sdsc.edu',\n",
       "    'uname Linux x86_64 3.10.0-957.12.2.el7.x86_64 #1 SMP Tue May 14 21:24:32 UTC 2019',\n",
       "    'uptime 4082659']}}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_to_info_dict( temp_loc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse file in comet_accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data = [ locs['job_info']+'/'+stamp for stamp in listdir(locs['job_info']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = job_data[0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_sample = open_txt( temp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = job_sample[0]\n",
    "rules.split(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_sample[1].split(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_sample[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_by_date = {}\n",
    "unsaved = []\n",
    "\n",
    "for date in job_data:\n",
    "    try:\n",
    "        \n",
    "        # skip alt files\n",
    "        #check_stamp = int( date[-14] )\n",
    "        \n",
    "        # read in file contents\n",
    "        contents = open_txt( date )\n",
    "    \n",
    "        # formatting\n",
    "        label = date[-14:-4]\n",
    "        rules = contents[0]\n",
    "        jobs = contents[1:]\n",
    "        \n",
    "        # template to save\n",
    "        nodes_by_date[ label ] = {}\n",
    "        nodes_by_date[ label ][\"multiple\"] = {}\n",
    "        nodes_by_date[ label ][\"rules\"] = rules\n",
    "        \n",
    "        # run through lines in file\n",
    "        for job in jobs:\n",
    "            line = job.split(\"|\")\n",
    "            node = line[-1]\n",
    "            info = info_dict( rules, line )\n",
    "            \n",
    "            # save multiple node jobs to specified loc\n",
    "            if len(node) > 12:                \n",
    "                nodes_by_date[ label ][ \"multiple\" ][ node ] = info\n",
    "            \n",
    "            else:\n",
    "                nodes_by_date[ label ][ node[:11] ] = info\n",
    "                \n",
    "    except:\n",
    "        unsaved.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_by_date['2019-09-19']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse file in archive of archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aofa_data = [ locs['aofa']+'/'+stamp for stamp in listdir(locs['aofa']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = listdir(aofa_data[0])[0]\n",
    "aofa_data[0]+'/'+temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aofa_sample = unzip_txt( aofa_data[0]+'/'+temp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aofa_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREVIOUS PROCEDURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = clock.time()\n",
    "n = len(files)\n",
    "valid = []\n",
    "invalid = []\n",
    "scanned = 0\n",
    "\n",
    "for file_name in files[:10]:\n",
    "    scanned += 1\n",
    "    perc_scanned = np.round( scanned / n * 100, 2)\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    print \"Processing file {} of {} files \\t ({}% of total files)\".format( scanned, n, perc_scanned )\n",
    "        \n",
    "    try:\n",
    "        check_file = open( source_dir+file_name, 'r' )\n",
    "        \n",
    "        for line in check_file:\n",
    "            print(line)\n",
    "        \n",
    "        check_file.close()\n",
    "    except:\n",
    "        next\n",
    "#        job_obj = pickle.load( pickle_file )\n",
    "#        \n",
    "#        # Save data\n",
    "#        valid.append(job_obj)\n",
    "#        summary[date][\"Saved\"] += 1\n",
    "#        \n",
    "#        pickle_file.close()\n",
    "#            \n",
    "#    except:\n",
    "#        invalid.append( file_name )\n",
    "#            \n",
    "#    print\n",
    "#    print \"Run time: {}s\".format( np.round( clock.time() - t0, 1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(invalid) == len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of date directories in source_dir\n",
    "dates_list = [ date for date in listdir(source_dir) if len(listdir(source_dir+date)) > 0 ]\n",
    "all_files = [ source_dir+date+'/'+file_name for date in listdir(source_dir) for file_name in listdir(source_dir+date) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm Already Scanned Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected = [ file_name[12:19] for file_name in listdir('../data/raw') ]\n",
    "remaining = [ file_name for file_name in all_files if file_name[72:] not in collected ]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Total files:\\t\", len(all_files)\n",
    "print \"Collected:\\t\", len(collected)\n",
    "print \"--------------------------\"\n",
    "print \"Remaining:\\t\", len(remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_val( val ):\n",
    "    try:\n",
    "        val = float(val)\n",
    "        return val\n",
    "    except:\n",
    "        return 0\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "def convert_dt( val ):\n",
    "    return dt.datetime.utcfromtimestamp( val ).strftime( \"%Y-%m-%d %H:%M:%S\" )\n",
    "\n",
    "def get_schemas( job ):\n",
    "    return { stat:schema.keys() for stat,schema in job.schemas.items() }\n",
    "\n",
    "def get_indices( job, host ):\n",
    "    indices = []\n",
    "    stats = [ stat for stat in job.schemas.keys() if stat in host.stats.keys() ]\n",
    "    schemas = { stat:schema.keys() for stat,schema in job.schemas.items() }\n",
    "    cores = { stat:core.keys() for stat,core in host.stats.items() }\n",
    "    \n",
    "    for stat in stats:\n",
    "        for core in cores[stat]:\n",
    "            for schema in schemas[stat]:\n",
    "                indices.append( (stat,core,schema) )\n",
    "             \n",
    "    return indices\n",
    "\n",
    "def get_times( job, host ):\n",
    "    times = [ job.start_time ]\n",
    "    times.extend( host.times )\n",
    "    times.append( job.end_time )\n",
    "    return [ convert_dt(t) for t in times ]\n",
    "\n",
    "def clean_list( data_list ):\n",
    "    return [ check_val( x ) for x in data_list ]\n",
    "    \n",
    "def get_data( host, row_labels ):\n",
    "    data = { label:[] for label in row_labels }\n",
    "    \n",
    "    for stat,node in host.stats.items():\n",
    "        for core,matrix in node.items():\n",
    "            matrix = matrix.T\n",
    "            for i in range(len(matrix)):\n",
    "                data[stat,core] = clean_list( matrix[i] )\n",
    "    return data\n",
    "\n",
    "def fill_df( template_df, data_dict):\n",
    "    for row,data in data_dict.items():\n",
    "        template_df.loc[row].update( pd.Series(data) )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpicore ( job_df ):\n",
    "    data = job_df.loc['intel_hsw']\n",
    "    times = job_df.columns.tolist()\n",
    "    cpicore_list = []\n",
    "    \n",
    "    for i in range(1, len(times)):\n",
    "        chunk = data[times[:i+1]]\n",
    "        devices = { row : np.mean(col.values) for row,col in chunk.iterrows() }\n",
    "        avg_c = { key[0]:0 for key,val in devices.items() }\n",
    "        sum_avgs = 0\n",
    "        \n",
    "        for key,val in avg_c.items():\n",
    "            avg_c[ key ] = devices[ (key, 'CLOCKS_UNHALTED_CORE') ] / devices[ (key, 'INSTRUCTIONS_RETIRED') ]\n",
    "    \n",
    "        for key,val in avg_c.items():\n",
    "            sum_avgs += val\n",
    "            \n",
    "        cpicore_list.append(sum_avgs/24)\n",
    "    \n",
    "    return cpicore_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def find_next( current, unsorted ):\n",
    "#    target = current + 00:10:00\n",
    "#    found = unsorted[0]\n",
    "#    proximity = target - found\n",
    "#    \n",
    "#    if len(unsorted) > 1:\n",
    "#        for i in range(len(unsorted)):\n",
    "#            if target - unsorted[i] < proximity:\n",
    "#                found = unsorted[i]\n",
    "#                proximity = target - found\n",
    "#    return found\n",
    "#\n",
    "#def fill_sorted( start, unsorted ):\n",
    "#    sorted_list = []\n",
    "#    \n",
    "#    for i in range(len(unsorted)):\n",
    "#        current = sorted_list[i]\n",
    "#        next_time = find_next( current, unsorted )\n",
    "#        sorted_list[i+1] = next_time\n",
    "#        \n",
    "#def sort_times( job ):\n",
    "#    start = job.start\n",
    "#    mid = job.times\n",
    "#    end = job.end\n",
    "#    \n",
    "#    if start == end:\n",
    "#        return [start]\n",
    "#    elif len(mid) < 1:\n",
    "#        return [start, end]\n",
    "#    elif len(mid) < 2:\n",
    "#        return [start, mid[0], end]\n",
    "#    else:\n",
    "#        return fill_sorted( start, mid.append(end) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and open pickled job files\n",
    "**Process:**\n",
    "    - Iterate through the non-empty date folders available in source_dir\n",
    "    - A file is saved in valid_jobs if:\n",
    "        * The pickled file is a Job object\n",
    "    - Exceptions are skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = remaining\n",
    "n = len(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {}\n",
    "\n",
    "for file_name in cut:\n",
    "    date = file_name[61:71]\n",
    "    \n",
    "    if date in summary:\n",
    "        summary[date][\"Total\"] += 1\n",
    "    else:\n",
    "        summary[date] = {}\n",
    "        summary[date][\"Total\"] = 1\n",
    "        summary[date][\"Saved\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catch invalid files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = clock.time()\n",
    "valid = []\n",
    "invalid = []\n",
    "scanned = 0\n",
    "\n",
    "for file_name in cut:\n",
    "    date = file_name[61:71]\n",
    "    scanned += 1\n",
    "    perc_scanned = np.round( scanned / n * 100, 2)\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    print \"Processing file {} of {} files \\t ({}% of total files)\".format( scanned, n, perc_scanned )\n",
    "        \n",
    "    try:\n",
    "        pickle_file = open( file_name, 'rb' )\n",
    "        job_obj = pickle.load( pickle_file )\n",
    "        \n",
    "        # Save data\n",
    "        valid.append(job_obj)\n",
    "        summary[date][\"Saved\"] += 1\n",
    "        \n",
    "        pickle_file.close()\n",
    "            \n",
    "    except:\n",
    "        invalid.append( file_name )\n",
    "            \n",
    "    print\n",
    "    print \"Run time: {}s\".format( np.round( clock.time() - t0, 1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_out = '../src/data/summary_stats/raw_metrics/invalid.pkl'\n",
    "out_file = open(invalid_out, 'wb')\n",
    "pickle.dump(invalid, out_file)\n",
    "#invalid = pickle.load(open(invalid_out, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_out = '../src/data/summary_stats/raw_metrics/all_rows.pkl'\n",
    "out_file = open(all_rows_out, 'wb')\n",
    "pickle.dump(all_rows, out_file)\n",
    "#valid = pickle.load(open(valid_out, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Total job objects collected:\\t\", len(valid)\n",
    "print\n",
    "print \"\\tBreakdown of files\"\n",
    "print \"=========================\"\n",
    "\n",
    "for date,info in summary:\n",
    "    \n",
    "    print \"Date:\\t\", date\n",
    "    print info[\"Saved\"], \"files saved out of\", info[\"Total\"]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_objects = valid\n",
    "m = float(sum([len(job.hosts.keys()) for job in job_objects]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = len(job_objects)/2\n",
    "first = 0\n",
    "stop = len(job_objects)\n",
    "rem = stop - first\n",
    "\n",
    "print \"Total Jobs (this date):\\t\\t\", len(job_objects)\n",
    "print \"Total Host,Job Pairs:\\t\\t\", int(m)\n",
    "print(\"------------------------------------\")\n",
    "print \"Remaining jobs to scan:\\t\\t\", int(rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dfs = {}\n",
    "t0 = clock.time()\n",
    "total = 0\n",
    "current = 0\n",
    "\n",
    "for job_idx in range( first, stop ):\n",
    "    job = job_objects[ job_idx ]\n",
    "    schemas = get_schemas( job )\n",
    "    total += 1\n",
    "    \n",
    "    # support for tracking progress in below print statements\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # iterate through each host object job was run on\n",
    "    for host_name, host in job.hosts.iteritems():\n",
    "        print(\"Processing hosts for job {} of {} \\t ({}% of total)\".format(job_idx+1, stop, np.round( (current+first)/m*100, 2)))\n",
    "        current += 1\n",
    "        \n",
    "        # build MultiIndex for df \n",
    "        idx_labels = get_indices( job, host )\n",
    "        indices = pd.MultiIndex.from_tuples( idx_labels, names=['Stat', 'Device', 'Schema'] )\n",
    "                    \n",
    "        # process timestamps\n",
    "        times = get_times( job, host )\n",
    "    \n",
    "        # collect job data\n",
    "        data = get_data( host, idx_labels )\n",
    "        \n",
    "        # create df with MultiIndex, ordered times\n",
    "        df = pd.DataFrame( index=indices, columns=times ).sort_index()\n",
    "        \n",
    "        # fill df\n",
    "        for stat,devices in host.stats.items():\n",
    "            for device,data_matrix in devices.items():\n",
    "                for t_idx in range( len(data_matrix) ):\n",
    "                    timestamp = times[t_idx]\n",
    "                    \n",
    "                    for metric_idx in range( len(data_matrix[ t_idx ]) ):\n",
    "                        metric = schemas[stat][metric_idx]\n",
    "                        row_label = (stat,device,metric)\n",
    "                        datum = data_matrix[t_idx][metric_idx]\n",
    "                        \n",
    "                        df.loc[row_label][timestamp] = check_val(datum)\n",
    "        \n",
    "        # save job info from DataFrame to csv file\n",
    "        df.to_csv( path_or_buf=save_dir+\"{}_{}.csv\".format( host_name, job.id ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that no job was missed\n",
    "if total == ( stop-first ):\n",
    "    print \"Success!\"\n",
    "else:\n",
    "    print stop - first - total, \"jobs missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
