{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Data Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System dependencies\n",
    "from os import listdir\n",
    "import time as clock\n",
    "from datetime import timedelta\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data handling methods\n",
    "import prep_IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save to\n",
    "save_dir = '../data/continued/'\n",
    "\n",
    "# Directory of prev job scans\n",
    "id_dir = '../data/labels/IDs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of recent saved comet jobs\n",
    "source_dir = '/oasis/projects/nsf/sys200/stats/xsede_stats/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archive_of_archive',\n",
       " 'gordon_hostfile_logs',\n",
       " 'gordon_pickles',\n",
       " 'comet_accounting',\n",
       " 'gordon_accounting',\n",
       " 'comet_pickles',\n",
       " 'archive',\n",
       " '.htaccess',\n",
       " 'comet_hostfile_logs']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = listdir(source_dir)\n",
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/sys200/stats/xsede_stats/gordon_hostfile_logs\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/gordon_pickles\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/gordon_accounting\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/comet_pickles\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/.htaccess\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/comet_hostfile_logs\n"
     ]
    }
   ],
   "source": [
    "possible = [ source_dir+file_name for file_name in listdir(source_dir) ]\n",
    "\n",
    "for item in possible:\n",
    "    try:\n",
    "        listdir(item)\n",
    "    except:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = { 'aofa': source_dir+'archive_of_archive',\n",
    "         'job_info': source_dir+'comet_accounting',\n",
    "         'arc': source_dir+'archive'\n",
    "         #'host_info': source_dir+'comet_hostfile_logs',\n",
    "         #'old_pickles': source_dir+'comet_pickles'\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/sys200/stats/xsede_stats/archive_of_archive\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/comet_accounting\n",
      "/oasis/projects/nsf/sys200/stats/xsede_stats/archive\n"
     ]
    }
   ],
   "source": [
    "for key,loc in locs.items():\n",
    "    print(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_data = [ locs['arc']+'/'+host_dir+'/'+stamp \n",
    "            for host_dir in listdir(locs['arc'])\n",
    "            for stamp in listdir(locs['arc']+'/'+host_dir)  ]\n",
    "\n",
    "aofa_data = [ locs['aofa']+'/'+host_dir+'/'+stamp \n",
    "            for host_dir in listdir(locs['aofa'])\n",
    "            for stamp in listdir(locs['aofa']+'/'+host_dir)  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_info_locs = [ locs['job_info']+'/'+stamp for stamp in listdir(locs['job_info']) ]\n",
    "dates = [ loc[-14:-4] for loc in acct_info_locs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299014"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1809956"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aofa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acct_info_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opt: System Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process available system data ###\n",
    "#arc_hosts = [ host_to_info_dict( host_file ) for host_file in arc_data ]\n",
    "#aofa_hosts = [ host_to_info_dict( host_file ) for host_file in aofa_data ]\n",
    "#acct_dates = job_to_info_dict( acct_info_locs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process user compiled list of target samples ###\n",
    "chosen = [('comet-14-72,comet-19-19,comet-28-55', '2020-05-13T02:11:38', '2020-05-13T05:07:34', '33301074'),\n",
    "         ('comet-30-10', '2020-05-13T10:50:08', '2020-05-13T10:50:12', '33321014'),\n",
    "          ('comet-06-46,comet-12-52,comet-22-[39,64]', '2020-05-12T06:40:43', '2020-05-13T14:27:44', '33283100'),\n",
    "          ('comet-21-07', '2020-05-27T08:49:26', '2020-05-28T01:27:49', '33637231'),\n",
    "          ('comet-22-48', '2020-05-27T09:32:35', '2020-05-28T00:25:06', '33637422'),\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [('comet-14-72,comet-19-19,comet-28-55',\n",
       "   '2020-05-13T02:11:38',\n",
       "   '2020-05-13T05:07:34',\n",
       "   '33301074'),\n",
       "  ('comet-30-10', '2020-05-13T10:50:08', '2020-05-13T10:50:12', '33321014'),\n",
       "  ('comet-06-46,comet-12-52,comet-22-[39,64]',\n",
       "   '2020-05-12T06:40:43',\n",
       "   '2020-05-13T14:27:44',\n",
       "   '33283100'),\n",
       "  ('comet-21-07', '2020-05-27T08:49:26', '2020-05-28T01:27:49', '33637231'),\n",
       "  ('comet-22-48', '2020-05-27T09:32:35', '2020-05-28T00:25:06', '33637422')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_list = prep_IO.search(mode='l',from_list=chosen)\n",
    "found_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys = []\n",
    "\n",
    "#for item in chosen:\n",
    "#    jobid = item[3]\n",
    "#    nodelist = format_nodelist( item[0] )\n",
    "#    start = str(get_stamp( item[1] ))\n",
    "#    end = str(get_stamp( item[2] ))\n",
    "    \n",
    "#    for node in nodelist:\n",
    "#        keys.append( ( node, start, end, jobid) )\n",
    "    \n",
    "    #print(\"Jobid:\\t\", jobid)\n",
    "    #print(\"Hosts:\\t\", nodelist)\n",
    "    #print(\"Start(ep):\\t\", start)\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chosen_locs = []\n",
    "#\n",
    "#for loc in aofa_data:\n",
    "#    for key in keys:\n",
    "#        host = key[0]\n",
    "#        t_n = key[2]\n",
    "#        \n",
    "#        if (host in loc) and t_n is not '0':\n",
    "#            if t_n in loc or t_n[:-2] in loc:\n",
    "#                chosen_locs.append(loc)\n",
    "#\n",
    "#for loc in arc_data:\n",
    "#    for key in keys:\n",
    "#        host = key[0]\n",
    "#        t_n = key[2]\n",
    "#        \n",
    "#        if (host in loc) and t_n is not '0':\n",
    "#            if t_n in loc or t_n[:-2] in loc:\n",
    "#                chosen_locs.append(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(chosen_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chosen_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System dependencies\n",
    "from os import listdir\n",
    "import time as clock\n",
    "from datetime import timedelta\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "# Data manipulation dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "### Prep Cleaning\n",
    "\n",
    "def get_time( spec=None ):\n",
    "    if type(spec) is str:\n",
    "        spec = float( spec )\n",
    "    \n",
    "    return clock.strftime(\"%Y-%m-%dT%H:%M:%S\", clock.localtime( spec ))\n",
    "\n",
    "def get_stamp( spec ):\n",
    "    try:\n",
    "        sf = \"%Y-%m-%dT%H:%M:%S\"\n",
    "        return int(clock.mktime( clock.strptime( spec, sf ) ))\n",
    "    except:\n",
    "        try:\n",
    "            sf = \"'%Y-%m-%dT%H:%M:%S'\"\n",
    "            return int(clock.mktime( clock.strptime( spec, sf ) ))\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "def check_static( alist ):\n",
    "    return alist[1:] == alist[:-1]\n",
    "\n",
    "def check_header( line ):\n",
    "    if line.find(\" \") < 0:\n",
    "        try:\n",
    "            return line[0] == '%'\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "    else:\n",
    "        chunks = line.split(\" \")\n",
    "        try:\n",
    "            return (chunks[0][0] == '%') or ( chunks[2].find(\"comet\") >= 0 )\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "#def group_from_txt(  ):\n",
    "#    \n",
    "\n",
    "def check_job( chunk ):\n",
    "    return chunk.find(\"-\") == -1\n",
    "\n",
    "def open_txt( txt_file ):\n",
    "    \n",
    "    with open( txt_file, \"rt\" ) as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def unzip_txt( gzipped ):\n",
    "    \n",
    "    with gzip.open( gzipped, 'rt') as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def group_from_txt( txt_file ):\n",
    "    lines = open_txt( txt_file )\n",
    "    group = []\n",
    "    \n",
    "    for line in lines:\n",
    "        chunks = line.split(\" \")\n",
    "        nodelist_r = chunks[0]\n",
    "        nodelist = format_nodelist( chunks[0] )\n",
    "        start = get_stamp( chunks[1] )\n",
    "        end = get_stamp( chunks[2] )\n",
    "        \n",
    "        item = ( nodelist, start, end )\n",
    "        group.append(item)\n",
    "        \n",
    "    return group\n",
    "\n",
    "def quick_save( obj, label=get_time() ):\n",
    "    \n",
    "    try:\n",
    "        out_file = open( label, 'wb')\n",
    "        pickle.dump( obj, out_file)\n",
    "        \n",
    "        # double check save\n",
    "        check_cpicore_set = pickle.load(open(cpiset_out, 'rb'))\n",
    "        check_cpicore_set = None\n",
    "        \n",
    "    except:\n",
    "        \"There was a problem pickling the object - Save manually.\"\n",
    "\n",
    "####Data Munging\n",
    "\n",
    "def info_dict( rules, info ):\n",
    "    rules_list = rules.split(\"|\")\n",
    "    \n",
    "    if len(rules_list) != len(info):\n",
    "        return {}\n",
    "    \n",
    "    else:\n",
    "        return { rules_list[i]:info[i] for i in range(len(rules_list)) }\n",
    "\n",
    "def host_to_info_dict( zip_txt ):\n",
    "    contents = unzip_txt( zip_txt )\n",
    "    host_name = contents[1].partition(\" \")[2][:11]\n",
    "    out_dict = { host_name: {} }\n",
    "    host_info = {}\n",
    "    info_dict = { \"Data\":{},\n",
    "                    \"Job\":\"N/A\",\n",
    "                    \"Schemas\":{},\n",
    "                    \"Specs\":[]\n",
    "                }\n",
    "    \n",
    "    for line in contents:\n",
    "            \n",
    "        if line[0] == \"$\":\n",
    "            info_dict[\"Specs\"].append( format_spec( line ) )\n",
    "            \n",
    "        elif line[0] == \"!\":\n",
    "            info_dict[\"Schemas\"].update( format_schema( line ) )\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if (len(line) > 0) and (len(line) < 3 or check_header( line )):\n",
    "                header_dict = format_header( line )\n",
    "                \n",
    "                if header_dict:\n",
    "                    t = header_dict[\"Timestamp\"]\n",
    "                    host_info[ t ] = {}\n",
    "                    \n",
    "                    if check_job( header_dict[\"Jobid\"] ):\n",
    "                        info_dict[\"Job\"] = { \"Jobid\": header_dict[\"Jobid\"] } \n",
    "                    \n",
    "            else:\n",
    "                incoming = format_data( line )\n",
    "                info_dict[\"Data\"].update(incoming)\n",
    "                \n",
    "                host_info[t].update( info_dict )\n",
    "                \n",
    "    out_dict[host_name].update( host_info )\n",
    "    \n",
    "    return out_dict\n",
    "\n",
    "def job_to_info_dict( txt_file_list ):\n",
    "    nodes_by_date = {}\n",
    "    unsaved = []\n",
    "\n",
    "    for date in txt_file_list:\n",
    "        try:\n",
    "            # skip alt files\n",
    "            #check_stamp = int( date[-14] )\n",
    "            \n",
    "            # read in file contents\n",
    "            contents = open_txt( date )\n",
    "            \n",
    "            # formatting\n",
    "            label = date[-14:-4]\n",
    "            rules = contents[0]\n",
    "            jobs = contents[1:]\n",
    "            \n",
    "            # template to save\n",
    "            nodes_by_date[ label ] = {}\n",
    "            nodes_by_date[ label ][\"multiple\"] = {}\n",
    "            nodes_by_date[ label ][\"rules\"] = rules\n",
    "            \n",
    "            # run through lines in file\n",
    "            for job in jobs:\n",
    "                line = job.split(\"|\")\n",
    "                node = line[-1]\n",
    "                info = info_dict( rules, line )\n",
    "                \n",
    "                # save multiple node jobs to specified loc\n",
    "                if len(node) > 12:\n",
    "                    nodes = format_nodelist( info )\n",
    "                    for node in nodes:\n",
    "                        nodes_by_date[ label ][ \"multiple\" ][ node ] = info\n",
    "                \n",
    "                else:\n",
    "                    nodes_by_date[ label ][ node[:11] ] = info\n",
    "        except:\n",
    "            unsaved.append(date)\n",
    "            \n",
    "    \n",
    "    return nodes_by_date, unsaved\n",
    "\n",
    "####Formatting\n",
    "\n",
    "def format_header( line ):\n",
    "    chunks = line.split(\" \")\n",
    "    \n",
    "    try:\n",
    "        if chunks[0][0] == '%':\n",
    "            return {}\n",
    "        else:\n",
    "            return { \"Timestamp\": get_time( chunks[0] ), \n",
    "                     \"Jobid\": chunks[1],\n",
    "                     \"Host\": chunks[2][:11] }\n",
    "        \n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "def format_nodelist( nodelist ):\n",
    "    purged = nodelist.replace('[','').replace(']','').replace(',','-').replace('-','').split(\"comet\")[1:]\n",
    "    nodes = []\n",
    "    \n",
    "    for item in purged:\n",
    "        base = item[:2]\n",
    "        prev = 2\n",
    "        \n",
    "        for i in range( 4,len(item)+1,2 ):\n",
    "            node = 'comet' + '-' + base + '-' + item[ prev:i ]\n",
    "            nodes.append(node)\n",
    "            prev = i\n",
    "    return nodes\n",
    "\n",
    "def format_spec( line ):\n",
    "    return line[1:-1]\n",
    "\n",
    "def format_data( line ):\n",
    "    chunks = line.split(\" \")\n",
    "    \n",
    "    stat = chunks[0]\n",
    "    dev = chunks[1]\n",
    "    data = chunks[2:-1]\n",
    "    \n",
    "    return { (stat,dev): data }\n",
    "\n",
    "def format_schema( line ):\n",
    "    chunks = line.partition(\" \")\n",
    "    stat = chunks[0][1:]\n",
    "    \n",
    "    temp_sch = chunks[2:][0][:-1].replace(\",E\",\"\").replace(\",C\",\"\").split(\" \")\n",
    "    fin_sch = []\n",
    "    \n",
    "    for item in temp_sch:\n",
    "        \n",
    "        if item.find(\"=\") > -1:\n",
    "            new = item.replace(\",\",\"(\") + \")\"\n",
    "            fin_sch.append( new )\n",
    "        \n",
    "        else:\n",
    "            fin_sch.append( item )\n",
    "    \n",
    "    return { stat:fin_sch }\n",
    "\n",
    "def separate_nodes( search_tup ):\n",
    "    nl = search_tup[0]\n",
    "    t_0 = search_tup[1]\n",
    "    t_n = search_tup[2]\n",
    "    exp_list = []\n",
    "    \n",
    "    if len( search_tup ) > 3:\n",
    "        rem = search_tup[3:]\n",
    "    \n",
    "    for node in nl:\n",
    "        if len(search_tup) > 3:\n",
    "            exp_list.append( (node,t_0,t_n,rem) )\n",
    "        else:\n",
    "            exp_list.append( (node,t_0,t_n) )\n",
    "    \n",
    "    return exp_list\n",
    "    \n",
    "def from_list( chunks ):\n",
    "    nl_i = chunks.index(\"comet\")\n",
    "    nl = chunks[ nl_i ]\n",
    "    t_n = ''\n",
    "    \n",
    "    if nl_i == 0:\n",
    "        t_0 = chunks[1]\n",
    "    else:\n",
    "        t_0 = chunks[0]\n",
    "    \n",
    "    try:\n",
    "        for i in range(len(chunks)):\n",
    "            if chunks[ i ] < t_0:\n",
    "                t_0 = chunks[ i ]\n",
    "            elif chunks[ i ] > t_0 and (t_n == '' or t_n < chunks[ i ]):\n",
    "                t_n = chunks[ i ]                          \n",
    "    except:\n",
    "        next\n",
    "    \n",
    "    if len(chunks) > 3:\n",
    "        rem = [ e for e in chunks if (e is not nl) and (e not in ts) ]\n",
    "        return nl,ts,rem\n",
    "    else:\n",
    "        return nl,ts\n",
    "    return (nl, t_0, t_n)\n",
    "\n",
    "def from_tup( list_i ):\n",
    "    nl = ''\n",
    "    ts = []\n",
    "\n",
    "    for e in list_i:\n",
    "        if 'comet' in e:\n",
    "            nl = e\n",
    "        if 'T' in e:\n",
    "            try:\n",
    "                if get_stamp(e) is not '0':\n",
    "                    ts.append(t)\n",
    "            except:\n",
    "                next\n",
    "    \n",
    "    if len(list_i) > 3:\n",
    "        rem = [ e for e in list_i if (e is not nl) and (e not in ts) ]\n",
    "        return nl,sorted(ts),rem\n",
    "    else:\n",
    "        return nl,sorted(ts)\n",
    "\n",
    "def sort_input( aline ):\n",
    "    if type( aline ) is list:\n",
    "        return from_list(aline)\n",
    "    if type( aline ) is tuple:\n",
    "        return from_tup(aline)\n",
    "    \n",
    "def format_search_tup( line ):\n",
    "    \n",
    "    if len(line) > 1:\n",
    "        search_i = sort_input( line )\n",
    "        \n",
    "        nodelist = format_nodelist( search_i[0] )\n",
    "        start = get_stamp( search_i[1] )\n",
    "        end = get_stamp( search_i[2] )\n",
    "        \n",
    "        if len(search_i) > 3:\n",
    "            return nodelist,start,end,search_i[3:]\n",
    "        else:\n",
    "            return nodelist,start,end\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "####Data analysis\n",
    "\n",
    "def timely_dict( host_data, host_name ):\n",
    "    stamps = list(host_data[ host_name ].keys())\n",
    "    schemas = host_data[ host_name ][ stamps[0] ][\"Schemas\"]\n",
    "    timely_data = []\n",
    "    \n",
    "    for stamp in stamps:\n",
    "        for key,data in host_data[ host_name ][ stamp ][\"Data\"].items():\n",
    "            \n",
    "            stat = key[0]\n",
    "            dev = key[1]\n",
    "            \n",
    "            for i in range(len(data)):\n",
    "                metric = schemas[stat][i]\n",
    "            \n",
    "            info = (stat, metric, dev, int(data[i]), stamp)\n",
    "            timely_data.append( info )\n",
    "    \n",
    "    return timely_data\n",
    "\n",
    "# PARAMETERS:\n",
    "# 's/e' single search from start/end (manual)\n",
    "#       ie) \"Start, End: 2020-01-03T20:34:47, 2020-01-05T08:15:18\"\n",
    "# 's' single search from nodelist%start%end (manual)\n",
    "#       ie) \"NL, Start, End: comet-05-12 2020-03-03T20:34:47 2020-03-05T08:15:18\"\n",
    "#       ie) \"NL, Start, End: comet-05-[12,16] 2020-03-03T20:34:47 2020-03-05T08:15:18\"\n",
    "# 'l' repeated search from nodelist%start%end strings or (nodelist,start,end) tuples (from list)\n",
    "#       ie) myJobList = [ \"comet-05-12 2020-03-03T20:34:47 2020-03-05T08:15:18\",\n",
    "#                          (comet-05-12, 2020-03-03T20:34:47, 2020-03-05T08:15:18)   ]\n",
    "#           search( mode='l', myJobList )\n",
    "# 'f' repeated search from nodelist%start%end (from file)\n",
    "#       ie) \"Text file: your_search_file.txt\"  (Note: Mismatched file contents ignored)\n",
    "def search( mode=['s/e', 's', 'l','f'], from_list=False ):\n",
    "    \n",
    "    if mode == 's/e':\n",
    "        t_0,t_n = input(\"Start, End:\").replace(\",\", \"\").split(\" \")\n",
    "        start = get_stamp( t_0 )\n",
    "        end=get_stamp( t_n )\n",
    "        return start,end\n",
    "    \n",
    "    elif mode == 's':\n",
    "        line = input(\"NL, Start, End:\").replace(\",\", \"\").split(\" \")\n",
    "        line_tup = format_search_tup( line )\n",
    "        return line_tup\n",
    "\n",
    "    elif mode == 'l' and type(from_list) is list:\n",
    "        try:\n",
    "            out_list = []\n",
    "            dropped = []\n",
    "            \n",
    "            for item in from_list:\n",
    "                try:\n",
    "                    item_tup = format_search_tup( item )\n",
    "                    if len(item_tup[0] == 1):\n",
    "                        out_list.append( item_tup )\n",
    "                    else:\n",
    "                        expanded_tups = separate_nodes(item_tup)\n",
    "                        out_list = out_list + expanded_tups\n",
    "                except:\n",
    "                    dropped.append(item)\n",
    "            return out_list,dropped\n",
    "        except:\n",
    "            \"Unable to process variable passed to function. All items in list should be in\"\n",
    "    \n",
    "    elif mode == 'f':\n",
    "        search_list = group_from_text( input(\"Text file:\") )\n",
    "        return search_list\n",
    "    \n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
