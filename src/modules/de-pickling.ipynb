{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle file dependencies\n",
    "from tacc_stats.pickler.job_stats import Job\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System dependencies\n",
    "from os import listdir\n",
    "import time as clock\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of all pickled jobs via comet\n",
    "# new_dir = '/oasis/projects/nsf/sys200/stats/xsede_stats/archive'\n",
    "source_dir = '/oasis/projects/nsf/sys200/tcooper/xsede_stats/comet_pickles/'\n",
    "\n",
    "# Directory to save to\n",
    "save_dir = '../data/continued/'\n",
    "\n",
    "# Directory of prev job scans\n",
    "id_dir = '../data/labels/IDs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of date directories in source_dir\n",
    "dates_list = [ date for date in listdir(source_dir) if len(listdir(source_dir+date)) > 0 ]\n",
    "all_files = [ source_dir+date+'/'+file_name for date in listdir(source_dir) for file_name in listdir(source_dir+date) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm Already Scanned Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected = [ file_name[12:19] for file_name in listdir('../data/raw') ]\n",
    "remaining = [ file_name for file_name in all_files if file_name[72:] not in collected ]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files:\t2199159\n",
      "Collected:\t5398\n",
      "--------------------------\n",
      "Remaining:\t2194279\n"
     ]
    }
   ],
   "source": [
    "print \"Total files:\\t\", len(all_files)\n",
    "print \"Collected:\\t\", len(collected)\n",
    "print \"--------------------------\"\n",
    "print \"Remaining:\\t\", len(remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_val( val ):\n",
    "    try:\n",
    "        val = float(val)\n",
    "        return val\n",
    "    except:\n",
    "        return 0\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "def convert_dt( val ):\n",
    "    return dt.datetime.utcfromtimestamp( val ).strftime( \"%Y-%m-%d %H:%M:%S\" )\n",
    "\n",
    "def get_schemas( job ):\n",
    "    return { stat:schema.keys() for stat,schema in job.schemas.items() }\n",
    "\n",
    "def get_indices( job, host ):\n",
    "    indices = []\n",
    "    stats = [ stat for stat in job.schemas.keys() if stat in host.stats.keys() ]\n",
    "    schemas = { stat:schema.keys() for stat,schema in job.schemas.items() }\n",
    "    cores = { stat:core.keys() for stat,core in host.stats.items() }\n",
    "    \n",
    "    for stat in stats:\n",
    "        for core in cores[stat]:\n",
    "            for schema in schemas[stat]:\n",
    "                indices.append( (stat,core,schema) )\n",
    "             \n",
    "    return indices\n",
    "\n",
    "def get_times( job, host ):\n",
    "    times = [ job.start_time ]\n",
    "    times.extend( host.times )\n",
    "    times.append( job.end_time )\n",
    "    return [ convert_dt(t) for t in times ]\n",
    "\n",
    "def clean_list( data_list ):\n",
    "    return [ check_val( x ) for x in data_list ]\n",
    "    \n",
    "def get_data( host, row_labels ):\n",
    "    data = { label:[] for label in row_labels }\n",
    "    \n",
    "    for stat,node in host.stats.items():\n",
    "        for core,matrix in node.items():\n",
    "            matrix = matrix.T\n",
    "            for i in range(len(matrix)):\n",
    "                data[stat,core] = clean_list( matrix[i] )\n",
    "    return data\n",
    "\n",
    "def fill_df( template_df, data_dict):\n",
    "    for row,data in data_dict.items():\n",
    "        template_df.loc[row].update( pd.Series(data) )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpicore ( job_df ):\n",
    "    data = job_df.loc['intel_hsw']\n",
    "    times = job_df.columns.tolist()\n",
    "    cpicore_list = []\n",
    "    \n",
    "    for i in range(1, len(times)):\n",
    "        chunk = data[times[:i+1]]\n",
    "        devices = { row : np.mean(col.values) for row,col in chunk.iterrows() }\n",
    "        avg_c = { key[0]:0 for key,val in devices.items() }\n",
    "        sum_avgs = 0\n",
    "        \n",
    "        for key,val in avg_c.items():\n",
    "            avg_c[ key ] = devices[ (key, 'CLOCKS_UNHALTED_CORE') ] / devices[ (key, 'INSTRUCTIONS_RETIRED') ]\n",
    "    \n",
    "        for key,val in avg_c.items():\n",
    "            sum_avgs += val\n",
    "            \n",
    "        cpicore_list.append(sum_avgs/24)\n",
    "    \n",
    "    return cpicore_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def find_next( current, unsorted ):\n",
    "#    target = current + 00:10:00\n",
    "#    found = unsorted[0]\n",
    "#    proximity = target - found\n",
    "#    \n",
    "#    if len(unsorted) > 1:\n",
    "#        for i in range(len(unsorted)):\n",
    "#            if target - unsorted[i] < proximity:\n",
    "#                found = unsorted[i]\n",
    "#                proximity = target - found\n",
    "#    return found\n",
    "#\n",
    "#def fill_sorted( start, unsorted ):\n",
    "#    sorted_list = []\n",
    "#    \n",
    "#    for i in range(len(unsorted)):\n",
    "#        current = sorted_list[i]\n",
    "#        next_time = find_next( current, unsorted )\n",
    "#        sorted_list[i+1] = next_time\n",
    "#        \n",
    "#def sort_times( job ):\n",
    "#    start = job.start\n",
    "#    mid = job.times\n",
    "#    end = job.end\n",
    "#    \n",
    "#    if start == end:\n",
    "#        return [start]\n",
    "#    elif len(mid) < 1:\n",
    "#        return [start, end]\n",
    "#    elif len(mid) < 2:\n",
    "#        return [start, mid[0], end]\n",
    "#    else:\n",
    "#        return fill_sorted( start, mid.append(end) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and open pickled job files\n",
    "**Process:**\n",
    "    - Iterate through the non-empty date folders available in source_dir\n",
    "    - A file is saved in valid_jobs if:\n",
    "        * The pickled file is a Job object\n",
    "    - Exceptions are skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = remaining\n",
    "n = len(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {}\n",
    "\n",
    "for file_name in cut:\n",
    "    date = file_name[61:71]\n",
    "    \n",
    "    if date in summary:\n",
    "        summary[date][\"Total\"] += 1\n",
    "    else:\n",
    "        summary[date] = {}\n",
    "        summary[date][\"Total\"] = 1\n",
    "        summary[date][\"Saved\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2016-08-19': {'Saved': 0, 'Total': 7732},\n",
       " '2016-08-20': {'Saved': 0, 'Total': 9610},\n",
       " '2016-08-21': {'Saved': 0, 'Total': 12071},\n",
       " '2016-08-22': {'Saved': 0, 'Total': 15356},\n",
       " '2016-08-23': {'Saved': 0, 'Total': 22251},\n",
       " '2016-08-24': {'Saved': 0, 'Total': 13709},\n",
       " '2016-08-25': {'Saved': 0, 'Total': 12779},\n",
       " '2016-08-26': {'Saved': 0, 'Total': 12042},\n",
       " '2016-08-27': {'Saved': 0, 'Total': 11752},\n",
       " '2016-08-28': {'Saved': 0, 'Total': 11869},\n",
       " '2016-08-29': {'Saved': 0, 'Total': 8462},\n",
       " '2016-08-30': {'Saved': 0, 'Total': 3106},\n",
       " '2016-08-31': {'Saved': 0, 'Total': 11619},\n",
       " '2016-09-01': {'Saved': 0, 'Total': 11086},\n",
       " '2016-09-02': {'Saved': 0, 'Total': 15194},\n",
       " '2016-09-03': {'Saved': 0, 'Total': 12176},\n",
       " '2016-09-04': {'Saved': 0, 'Total': 32656},\n",
       " '2016-09-05': {'Saved': 0, 'Total': 51049},\n",
       " '2016-09-06': {'Saved': 0, 'Total': 39128},\n",
       " '2016-09-07': {'Saved': 0, 'Total': 13329},\n",
       " '2016-09-08': {'Saved': 0, 'Total': 17233},\n",
       " '2016-09-09': {'Saved': 0, 'Total': 24291},\n",
       " '2016-09-10': {'Saved': 0, 'Total': 15268},\n",
       " '2016-09-11': {'Saved': 0, 'Total': 6691},\n",
       " '2016-09-12': {'Saved': 0, 'Total': 6424},\n",
       " '2016-09-13': {'Saved': 0, 'Total': 6886},\n",
       " '2016-09-14': {'Saved': 0, 'Total': 6761},\n",
       " '2016-09-15': {'Saved': 0, 'Total': 6267},\n",
       " '2016-09-16': {'Saved': 0, 'Total': 12137},\n",
       " '2016-09-17': {'Saved': 0, 'Total': 6869},\n",
       " '2016-09-18': {'Saved': 0, 'Total': 11211},\n",
       " '2016-09-19': {'Saved': 0, 'Total': 20616},\n",
       " '2016-09-20': {'Saved': 0, 'Total': 16536},\n",
       " '2016-09-21': {'Saved': 0, 'Total': 23656},\n",
       " '2016-09-22': {'Saved': 0, 'Total': 25141},\n",
       " '2016-09-23': {'Saved': 0, 'Total': 22055},\n",
       " '2016-09-24': {'Saved': 0, 'Total': 7251},\n",
       " '2016-09-25': {'Saved': 0, 'Total': 12621},\n",
       " '2016-09-26': {'Saved': 0, 'Total': 26858},\n",
       " '2016-09-27': {'Saved': 0, 'Total': 19603},\n",
       " '2016-09-28': {'Saved': 0, 'Total': 25572},\n",
       " '2016-09-29': {'Saved': 0, 'Total': 27479},\n",
       " '2016-09-30': {'Saved': 0, 'Total': 20393},\n",
       " '2016-10-01': {'Saved': 0, 'Total': 13034},\n",
       " '2016-10-02': {'Saved': 0, 'Total': 8080},\n",
       " '2016-10-03': {'Saved': 0, 'Total': 14830},\n",
       " '2016-10-04': {'Saved': 0, 'Total': 40094},\n",
       " '2016-10-05': {'Saved': 0, 'Total': 36918},\n",
       " '2016-10-06': {'Saved': 0, 'Total': 37437},\n",
       " '2016-10-07': {'Saved': 0, 'Total': 22817},\n",
       " '2016-10-08': {'Saved': 0, 'Total': 25226},\n",
       " '2016-10-09': {'Saved': 0, 'Total': 34424},\n",
       " '2016-10-10': {'Saved': 0, 'Total': 13989},\n",
       " '2016-10-11': {'Saved': 0, 'Total': 45320},\n",
       " '2016-10-12': {'Saved': 0, 'Total': 43052},\n",
       " '2016-10-13': {'Saved': 0, 'Total': 19283},\n",
       " '2016-10-14': {'Saved': 0, 'Total': 6502},\n",
       " '2016-10-15': {'Saved': 0, 'Total': 10835},\n",
       " '2016-10-16': {'Saved': 0, 'Total': 34214},\n",
       " '2016-10-17': {'Saved': 0, 'Total': 57835},\n",
       " '2016-10-18': {'Saved': 0, 'Total': 55665},\n",
       " '2016-10-19': {'Saved': 0, 'Total': 27058},\n",
       " '2016-10-20': {'Saved': 0, 'Total': 32943},\n",
       " '2016-10-21': {'Saved': 0, 'Total': 26408},\n",
       " '2016-10-22': {'Saved': 0, 'Total': 4656},\n",
       " '2016-10-23': {'Saved': 0, 'Total': 6641},\n",
       " '2016-10-24': {'Saved': 0, 'Total': 6800},\n",
       " '2016-10-25': {'Saved': 0, 'Total': 1033},\n",
       " '2016-10-26': {'Saved': 0, 'Total': 21287},\n",
       " '2016-10-27': {'Saved': 0, 'Total': 31159},\n",
       " '2016-10-28': {'Saved': 0, 'Total': 17441},\n",
       " '2016-10-29': {'Saved': 0, 'Total': 23460},\n",
       " '2016-10-30': {'Saved': 0, 'Total': 38128},\n",
       " '2016-10-31': {'Saved': 0, 'Total': 23854},\n",
       " '2016-11-01': {'Saved': 0, 'Total': 18180},\n",
       " '2016-11-02': {'Saved': 0, 'Total': 21863},\n",
       " '2016-11-03': {'Saved': 0, 'Total': 13828},\n",
       " '2016-11-04': {'Saved': 0, 'Total': 56271},\n",
       " '2016-11-05': {'Saved': 0, 'Total': 64663},\n",
       " '2016-11-06': {'Saved': 0, 'Total': 46037},\n",
       " '2016-11-07': {'Saved': 0, 'Total': 92256},\n",
       " '2016-11-08': {'Saved': 0, 'Total': 56032},\n",
       " '2016-11-09': {'Saved': 0, 'Total': 62299},\n",
       " '2016-11-10': {'Saved': 0, 'Total': 35923},\n",
       " '2016-11-11': {'Saved': 0, 'Total': 29591},\n",
       " '2016-11-12': {'Saved': 0, 'Total': 1628},\n",
       " '2016-11-13': {'Saved': 0, 'Total': 1860},\n",
       " '2016-11-14': {'Saved': 0, 'Total': 1580},\n",
       " '2016-11-15': {'Saved': 0, 'Total': 20326},\n",
       " '2016-11-16': {'Saved': 0, 'Total': 42965},\n",
       " '2016-11-17': {'Saved': 0, 'Total': 51477},\n",
       " '2016-11-18': {'Saved': 0, 'Total': 45446},\n",
       " '2016-11-19': {'Saved': 0, 'Total': 9629},\n",
       " '2016-11-20': {'Saved': 0, 'Total': 5258},\n",
       " '2016-11-21': {'Saved': 0, 'Total': 3563},\n",
       " '2016-11-22': {'Saved': 0, 'Total': 8320},\n",
       " '2016-11-23': {'Saved': 0, 'Total': 30066}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catch invalid files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1932842 of 2194279 files \t (0% of total files)\n",
      "\n",
      "Run time: 68903.8s\n"
     ]
    }
   ],
   "source": [
    "t0 = clock.time()\n",
    "valid = []\n",
    "invalid = []\n",
    "scanned = 0\n",
    "\n",
    "for file_name in cut:\n",
    "    date = file_name[61:71]\n",
    "    scanned += 1\n",
    "    perc_scanned = np.round( scanned / n * 100, 2)\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    print \"Processing file {} of {} files \\t ({}% of total files)\".format( scanned, n, perc_scanned )\n",
    "        \n",
    "    try:\n",
    "        pickle_file = open( file_name, 'rb' )\n",
    "        job_obj = pickle.load( pickle_file )\n",
    "        \n",
    "        # Save data\n",
    "        valid.append(job_obj)\n",
    "        summary[date][\"Saved\"] += 1\n",
    "        \n",
    "        pickle_file.close()\n",
    "            \n",
    "    except:\n",
    "        invalid.append( file_name )\n",
    "            \n",
    "    print\n",
    "    print \"Run time: {}s\".format( np.round( clock.time() - t0, 1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_out = '../src/data/summary_stats/raw_metrics/invalid.pkl'\n",
    "out_file = open(invalid_out, 'wb')\n",
    "pickle.dump(invalid, out_file)\n",
    "#invalid = pickle.load(open(invalid_out, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_out = '../src/data/summary_stats/raw_metrics/all_rows.pkl'\n",
    "out_file = open(all_rows_out, 'wb')\n",
    "pickle.dump(all_rows, out_file)\n",
    "#valid = pickle.load(open(valid_out, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Total job objects collected:\\t\", len(valid)\n",
    "print\n",
    "print \"\\tBreakdown of files\"\n",
    "print \"=========================\"\n",
    "\n",
    "for date,info in summary:\n",
    "    \n",
    "    print \"Date:\\t\", date\n",
    "    print info[\"Saved\"], \"files saved out of\", info[\"Total\"]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_objects = valid\n",
    "m = float(sum([len(job.hosts.keys()) for job in job_objects]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = len(job_objects)/2\n",
    "first = 0\n",
    "stop = len(job_objects)\n",
    "rem = stop - first\n",
    "\n",
    "print \"Total Jobs (this date):\\t\\t\", len(job_objects)\n",
    "print \"Total Host,Job Pairs:\\t\\t\", int(m)\n",
    "print(\"------------------------------------\")\n",
    "print \"Remaining jobs to scan:\\t\\t\", int(rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dfs = {}\n",
    "t0 = clock.time()\n",
    "total = 0\n",
    "current = 0\n",
    "\n",
    "for job_idx in range( first, stop ):\n",
    "    job = job_objects[ job_idx ]\n",
    "    schemas = get_schemas( job )\n",
    "    total += 1\n",
    "    \n",
    "    # support for tracking progress in below print statements\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # iterate through each host object job was run on\n",
    "    for host_name, host in job.hosts.iteritems():\n",
    "        print(\"Processing hosts for job {} of {} \\t ({}% of total)\".format(job_idx+1, stop, np.round( (current+first)/m*100, 2)))\n",
    "        current += 1\n",
    "        \n",
    "        # build MultiIndex for df \n",
    "        idx_labels = get_indices( job, host )\n",
    "        indices = pd.MultiIndex.from_tuples( idx_labels, names=['Stat', 'Device', 'Schema'] )\n",
    "                    \n",
    "        # process timestamps\n",
    "        times = get_times( job, host )\n",
    "    \n",
    "        # collect job data\n",
    "        data = get_data( host, idx_labels )\n",
    "        \n",
    "        # create df with MultiIndex, ordered times\n",
    "        df = pd.DataFrame( index=indices, columns=times ).sort_index()\n",
    "        \n",
    "        # fill df\n",
    "        for stat,devices in host.stats.items():\n",
    "            for device,data_matrix in devices.items():\n",
    "                for t_idx in range( len(data_matrix) ):\n",
    "                    timestamp = times[t_idx]\n",
    "                    \n",
    "                    for metric_idx in range( len(data_matrix[ t_idx ]) ):\n",
    "                        metric = schemas[stat][metric_idx]\n",
    "                        row_label = (stat,device,metric)\n",
    "                        datum = data_matrix[t_idx][metric_idx]\n",
    "                        \n",
    "                        df.loc[row_label][timestamp] = check_val(datum)\n",
    "        \n",
    "        # save job info from DataFrame to csv file\n",
    "        df.to_csv( path_or_buf=save_dir+\"{}_{}.csv\".format( host_name, job.id ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that no job was missed\n",
    "if total == ( stop-first ):\n",
    "    print \"Success!\"\n",
    "else:\n",
    "    print stop - first - total, \"jobs missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
