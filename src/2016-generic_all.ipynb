{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle file dependencies\n",
    "from tacc_stats.pickler.job_stats import Job\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System dependencies\n",
    "from os import listdir\n",
    "import time as clock\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation dependencies\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of all pickled jobs via comet\n",
    "# new_dir = '/oasis/projects/nsf/sys200/stats/xsede_stats/archive'\n",
    "source_dir = '/oasis/projects/nsf/sys200/tcooper/xsede_stats/comet_pickles/'\n",
    "\n",
    "# Directory of pre-cleaned job files\n",
    "dates_dir = './modules/data/dates(2016)/'\n",
    "\n",
    "# Directory to save to\n",
    "save_dir = './modules/data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_dict = {date:dates_dir+date for date in listdir(dates_dir)}\n",
    "dates_list = dates_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in jobs from cleaned jobs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_target( date_file ):\n",
    "    jobs_list = []\n",
    "    \n",
    "    # open file and read the content in a list\n",
    "    with open(date_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "        for jobid in lines:\n",
    "            current = jobid[:-1]\n",
    "            jobs_list.append(current)\n",
    "    \n",
    "    return jobs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and open pickled job files\n",
    "**Process:**\n",
    "    - Iterate through the non-empty date folders available in source_dir\n",
    "    - A file is saved in valid_jobs if:\n",
    "        * The pickled file is a Job object\n",
    "        * The job ran for more than 6 cycles (1 hour)\n",
    "        * The total number of jobs saved at the end of the previous date folder is less than 1000\n",
    "            _This is purely to keep the computations manageable according to compute time requested_\n",
    "    - Exceptions are skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_date = dates_list[13]\n",
    "target_file = dates_dict[ target_date ]\n",
    "jobids = prep_target( target_file )\n",
    "n = len(jobids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 197 of 197 files \t (100% of total files)\n",
      "\n",
      "Run time: 21.1s\n"
     ]
    }
   ],
   "source": [
    "job_objects = []\n",
    "t0 = clock.time()\n",
    "total = 0\n",
    "\n",
    "for jobid in jobids:\n",
    "    total += 1\n",
    "    clear_output(wait=True)\n",
    "    print(\"Processing file {} of {} files \\t ({}% of total files)\".format(total, n, np.round( total/n*100, 2)))\n",
    "    \n",
    "    pickle_file = open( source_dir+target_date+'/'+jobid, 'rb')\n",
    "    job_file = pickle.load(pickle_file)\n",
    "    job_objects.append(job_file)\n",
    "    pickle_file.close()      \n",
    "        \n",
    "    t2 = clock.time()\n",
    "    print\n",
    "    print(\"Run time: {}s\".format(np.round(t2-t0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops in loops in loops (Cleaning data)\n",
    "**Notes:**\n",
    "    - If a value is missing from the data, it will be replaced with '0' for the purpose of this project\n",
    "    - If a type of statistic was not collected on the job, that column is dropped from the DataFrame\n",
    "    - Two files are created during each iteration:\n",
    "         1) A .csv of the descriptive statistics for that host,job pair\n",
    "         2) A full .csv of the host,job data from the formatted DataFrame\n",
    "    - Naming convention: Files are labelled as '{host}_{jobid}' to support random lookup\n",
    "         * A job run on multiple host nodes is processed and saved with each individual host,job pair *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = float(sum([len(job.hosts.keys()) for job in job_objects]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs:\t\t\t197\n",
      "Total Host,Job Pairs:\t937\n"
     ]
    }
   ],
   "source": [
    "print \"Jobs:\\t\\t\\t\", len(job_objects)\n",
    "print \"Total Host,Job Pairs:\\t\", int(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hosts for job 86 of 197 \t (2.56% of total)\n",
      "Processing hosts for job 86 of 197 \t (2.67% of total)\n"
     ]
    }
   ],
   "source": [
    "schemas = {}\n",
    "schemas_devices = {}\n",
    "job_objects[0].schemas\n",
    "t0 = clock.time()\n",
    "total = 0\n",
    "current = 0\n",
    "\n",
    "for job in range(77,98):#len(job_objects) ):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # general job values\n",
    "    jobid = job_objects[job]\n",
    "    start = pd.to_datetime(round(jobid.start_time), unit='s').time()\n",
    "    end = pd.to_datetime(round(jobid.end_time), unit='s').time()\n",
    "    numCycles = len(jobid.times)\n",
    "    total += 1\n",
    "    type_avgs = {}\n",
    "    times = []\n",
    "    \n",
    "    ##################################\n",
    "    #  build master list of schemas  #\n",
    "    ##################################\n",
    "    for stat in jobid.schemas.keys():\n",
    "        if stat not in schemas.keys():\n",
    "             schemas[stat] = jobid.schemas[stat].keys()\n",
    "    \n",
    "    # iterate through each host object job was run on\n",
    "    for host_name, host in jobid.hosts.iteritems():\n",
    "        current += 1\n",
    "        \n",
    "        try:\n",
    "            print(\"Processing hosts for job {} of {} \\t ({}% of total)\".format(job+1, int(n), np.round( (current)/m*100, 2)))\n",
    "            \n",
    "            ##################################\n",
    "            #    convert timestamps to dt    #\n",
    "            ##################################\n",
    "            times.append(start)\n",
    "            for time in host.times:\n",
    "                times.append( pd.to_datetime(round(time), unit='s').time() )\n",
    "            times.append(end)\n",
    "            \n",
    "            ##################################\n",
    "            #  build master list of devices  #\n",
    "            ##################################\n",
    "            for stat in host.stats.keys():\n",
    "                if stat not in schemas_devices.keys():\n",
    "                    schemas_devices[stat] = host.stats[stat].keys()\n",
    "                 \n",
    "            indices_all = []\n",
    "            for stat,devices in schemas_devices.items():\n",
    "                for device in devices:\n",
    "                    for schema in schemas[stat]:\n",
    "                        indices_all.append( (stat,device,schema) )\n",
    "    \n",
    "            all_idx = pd.MultiIndex.from_tuples(indices_all, names=['Stat', 'Device', 'Schema'])  \n",
    "            all_df = pd.DataFrame( index=all_idx, columns=times ).sort_index()\n",
    "            \n",
    "            ##################################\n",
    "            #   iterate through host.stats   #\n",
    "            ##################################\n",
    "            for host_name,host in jobid.hosts.items():\n",
    "                for stat,devices in host.stats.items():\n",
    "                    for device,cycles in devices.items():\n",
    "                        for i in range(len(cycles)):\n",
    "                            for j in range(len(cycles[i])):\n",
    "                                try:\n",
    "                                    time = times[i]\n",
    "                                    schema = schemas[stat][j]\n",
    "                                    all_df.loc[(stat,device,schema),time] = cycles[i][j]\n",
    "                                except:\n",
    "                                    next\n",
    "            \n",
    "            all_df.to_csv(path_or_buf=save_dir+\"{}_{}.csv\".format( host_name, jobid.id ))\n",
    "            \n",
    "        except:\n",
    "            next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CUTOFF AT JOB 98 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that no job was missed\n",
    "if total == m:\n",
    "    print \"Success!\"\n",
    "else:\n",
    "    print len(job_objects) - total, \"jobs missing\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
