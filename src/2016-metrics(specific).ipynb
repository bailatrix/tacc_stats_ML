{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle file dependencies\n",
    "from tacc_stats.pickler.job_stats import Job\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System dependencies\n",
    "from os import listdir\n",
    "import time as clock\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import diff, amax, zeros, maximum, mean, isnan, trapz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of all pickled jobs via comet\n",
    "# new_dir = '/oasis/projects/nsf/sys200/stats/xsede_stats/archive'\n",
    "source_dir = '/oasis/projects/nsf/sys200/tcooper/xsede_stats/comet_pickles/'\n",
    "\n",
    "# Directory of pre-cleaned job files\n",
    "dates_dir = './modules/data/dates(2016)/'\n",
    "\n",
    "# Job ids we are looking for\n",
    "goals = [ '4721174', '4726104', '4678353' ]\n",
    "\n",
    "# Directory to save to\n",
    "save_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_dict = {date:dates_dir+date for date in listdir(dates_dir)}\n",
    "dates_list = dates_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in jobs from cleaned jobs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_target( date_file ):\n",
    "    jobs_list = []\n",
    "    \n",
    "    # open file and read the content in a list\n",
    "    with open(date_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "        for jobid in lines:\n",
    "            current = jobid[:-1]\n",
    "            jobs_list.append(current)\n",
    "    \n",
    "    return jobs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and open pickled job files\n",
    "**Process:**\n",
    "    - Iterate through the non-empty date folders available in source_dir\n",
    "    - A file is saved in valid_jobs if:\n",
    "        * The pickled file is a Job object\n",
    "        * The job ran for more than 6 cycles (1 hour)\n",
    "        * The total number of jobs saved at the end of the previous date folder is less than 1000\n",
    "            _This is purely to keep the computations manageable according to compute time requested_\n",
    "    - Exceptions are skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 37409 of 1513 files \t (2400% of total files)\n",
      "\n",
      "Run time: 2321.1s\n"
     ]
    }
   ],
   "source": [
    "job_objects = []\n",
    "t0 = clock.time()\n",
    "total = 0\n",
    "\n",
    "for i in range(len(dates_list)):\n",
    "    target_date = dates_list[i]\n",
    "    target_file = dates_dict[ target_date ]\n",
    "    jobids = prep_target( target_file )\n",
    "    n = len(jobids)\n",
    "\n",
    "    for jobid in jobids:\n",
    "        total += 1\n",
    "        clear_output(wait=True)\n",
    "        print(\"Processing file {} of {} files \\t ({}% of total files)\".format(total, n, np.round( total/n*100, 2)))\n",
    "        \n",
    "        pickle_file = open( source_dir+target_date+'/'+jobid, 'rb')\n",
    "        job_file = pickle.load(pickle_file)\n",
    "        if job_file.id in goals:\n",
    "            job_objects.append(job_file)\n",
    "        pickle_file.close()      \n",
    "            \n",
    "        t2 = clock.time()\n",
    "        print\n",
    "        print(\"Run time: {}s\".format(np.round(t2-t0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4678353\n",
      "<tacc_stats.pickler.job_stats.Job object at 0x2b77efb8daf0>\n",
      "\n",
      "4726104\n",
      "<tacc_stats.pickler.job_stats.Job object at 0x2b77efb14f30>\n",
      "\n",
      "4721174\n",
      "<tacc_stats.pickler.job_stats.Job object at 0x2b77effa91e8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for job in job_objects:\n",
    "    print(job.id)\n",
    "    print(job)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils prepping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utils():\n",
    "    def __init__(self, job):\n",
    "        freq_list = {\"intel_snb\" : 2.7, \"intel_ivb\" : 2.8, \"intel_hsw\" : 2.3,\n",
    "                     \"intel_bdw\" : 2.6, \"intel_knl\" : 1.4, \"intel_skx\" : 2.1}\n",
    "        imc_list  = [\"intel_snb_imc\", \"intel_ivb_imc\", \"intel_hsw_imc\",\n",
    "                     \"intel_bdw_imc\", \"intel_knl_mc_dclk\", \"intel_skx_imc\"]\n",
    "        cha_list = [\"intel_knl_cha\", \"intel_skx_cha\"]\n",
    "        self.job = job\n",
    "        self.nhosts = len(job.hosts.keys())\n",
    "        self.hostnames  = sorted(job.hosts.keys())\n",
    "        self.wayness = int(job.acct['cores'])/int(job.acct['nodes'])\n",
    "        self.hours = ((job.times[:] - job.times[0])/3600.).astype(float)\n",
    "        self.t = job.times\n",
    "        self.nt = len(job.times)\n",
    "        self.dt = (job.times[-1] - job.times[0]).astype(float)\n",
    "        for typename in  job.schemas.keys():\n",
    "            if typename in freq_list:\n",
    "                self.pmc  = typename\n",
    "                self.freq = freq_list[typename]\n",
    "            if typename in imc_list:\n",
    "                self.imc = typename\n",
    "            if typename in cha_list:\n",
    "                self.cha = typename\n",
    "    \n",
    "    def get_type(self, typename, aggregate = True):\n",
    "        if typename == \"imc\": typename = self.imc\n",
    "        if typename == \"pmc\": typename = self.pmc\n",
    "        if typename == \"cha\": typename = self.cha\n",
    "        if not typename: return\n",
    "    \n",
    "        schema = self.job.schemas[typename]\n",
    "        stats = {}\n",
    "        for hostname, host in self.job.hosts.items():\n",
    "            if aggregate:\n",
    "                stats[hostname] = 0\n",
    "                for devname in host.stats[typename]:\n",
    "                    stats[hostname] += host.stats[typename][devname].astype(float)\n",
    "            else:\n",
    "                stats[hostname] = {}\n",
    "                for devname in host.stats[typename]:\n",
    "                    stats[hostname][devname] = host.stats[typename][devname].astype(float)\n",
    "        return schema, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_list = []\n",
    "for job in job_objects:\n",
    "    u_list.append( utils(job) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.utils instance at 0x2b77efeff8c0>,\n",
       " <__main__.utils instance at 0x2b77f00f1ea8>,\n",
       " <__main__.utils instance at 0x2b77f00f1368>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatTitles ( df ):\n",
    "    return df.rename( columns={\"index\": \"Job Name\",\n",
    "              \"amd64_pmc\": \"AMD Opteron performance counters (per core)\",\n",
    "              \"intel_hsw\": \"Intel Haswell Processor (HSW) (per core)\",\n",
    "              \"intel_hsw_ht\": \"Intel Haswell Processor - Hyper-threaded (per logical core)\",\n",
    "              \"intel_nhm\": \"Intel Nehalem Processor (NHM) (per core)\",\n",
    "              \"intel_uncore\": \"Westmere Uncore (WTM) (per socket)\",\n",
    "              \"intel_snb\": \"Intel Sandy Brige (SNB) or Ivy Bridge (IVB) Processor (per core)\",\n",
    "              \"intel_rapl\": \"Running average power limit\",\n",
    "              \"intel_hsw_cbo\": \"Caching Agent (CBo) for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_pcu\": \"Power Control Unit for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_imc\": \"Integrated Memory Controller for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_qpi\": \"QPI Link Layer for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_hau\": \"Home Agent Unit for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_r2pci\": \"Ring to PCIe Agent for SNB (HSW) (per socket)\",\n",
    "              \"ib\": \"Infiniband usage (default)\",\n",
    "              \"ib_sw\": \"InfiniBand usage (sw)\",\n",
    "              \"ib_ext\": \"Infiniband usage (ext)\",\n",
    "              \"llite\": \"Lustre filesystem usage (per mount)\",\n",
    "              \"lnet\": \"Lustre network usage (lnet)\",\n",
    "              \"mdc\": \"Lustre network usage (mdc)\",\n",
    "              \"mic\": \"MIC scheduler account (per hardware thread)\",\n",
    "              \"osc\": \"Lustre filesystem usage (osc)\",\n",
    "              \"block\": \"Block device statistics (per device)\",\n",
    "              \"cpu\": \"Scheduler accounting (per CPU)\",\n",
    "              \"mem\": \"Memory usage (per socket)\",\n",
    "              \"net\": \"Network device usage (per device)\",\n",
    "              \"nfs\": \"NFS system usage\",\n",
    "              \"numa\": \"NUMA statistics (per socket)\",\n",
    "              \"proc\": \"Process specific data (MaxRSS, executable name etc.)\",\n",
    "              \"ps\": \"Process statistics\",\n",
    "              \"sysv_shm\": \"SysV shared memory segment usage\",\n",
    "              \"tmpfs\": \"Ram-backed filesystem usage (per mount)\",\n",
    "              \"vfs\": \"Dentry/file/inode cache usage\",\n",
    "              \"vm\": \"Virtual memory statistics\"\n",
    "                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Determined Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_blockbw(u):\n",
    "    schema, _stats = u.get_type(\"block\")\n",
    "    blockbw = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        blockbw += stats[-1, schema[\"rd_sectors\"].index] - stats[0, schema[\"rd_sectors\"].index] + \\\n",
    "               stats[-1, schema[\"wr_sectors\"].index] - stats[0, schema[\"wr_sectors\"].index]\n",
    "    return blockbw/(u.dt*u.nhosts*1024*1024)\n",
    "\n",
    "def avg_cpi(u):\n",
    "    schema, _stats = u.get_type(\"pmc\")\n",
    "    cycles = 0\n",
    "    instrs = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        cycles += stats[-1, schema[\"CLOCKS_UNHALTED_CORE\"].index] - \\\n",
    "            stats[0, schema[\"CLOCKS_UNHALTED_CORE\"].index]\n",
    "        instrs += stats[-1, schema[\"INSTRUCTIONS_RETIRED\"].index] - \\\n",
    "            stats[0, schema[\"INSTRUCTIONS_RETIRED\"].index] \n",
    "    return cycles/instrs\n",
    "\n",
    "def avg_freq(u):\n",
    "    schema, _stats = u.get_type(\"pmc\")\n",
    "    cycles = 0\n",
    "    cycles_ref = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        cycles += stats[-1, schema[\"CLOCKS_UNHALTED_CORE\"].index] - \\\n",
    "            stats[0, schema[\"CLOCKS_UNHALTED_CORE\"].index]\n",
    "        cycles_ref += stats[-1, schema[\"CLOCKS_UNHALTED_REF\"].index] - \\\n",
    "                stats[0, schema[\"CLOCKS_UNHALTED_REF\"].index] \n",
    "    return u.freq*cycles/cycles_ref\n",
    "\n",
    "def avg_cpuusage(u):\n",
    "    schema, _stats = u.get_type(\"cpu\")    \n",
    "    cpu = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        cpu += stats[-1, schema[\"user\"].index] - stats[0, schema[\"user\"].index]\n",
    "    return cpu/(u.dt*u.nhosts*100)\n",
    "\n",
    "def avg_ethbw(u):\n",
    "    schema, _stats = u.get_type(\"net\")\n",
    "    bw = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        bw += stats[-1, schema[\"rx_bytes\"].index] - stats[0, schema[\"rx_bytes\"].index] + \\\n",
    "              stats[-1, schema[\"tx_bytes\"].index] - stats[0, schema[\"tx_bytes\"].index]\n",
    "    return bw/(u.dt*u.nhosts*1024*1024)\n",
    "\n",
    "def avg_fabricbw(u):\n",
    "    avg = 0\n",
    "    try:\n",
    "        schema, _stats = u.get_type(\"ib_ext\")              \n",
    "        tb, rb = schema[\"port_xmit_data\"].index, schema[\"port_rcv_data\"].index\n",
    "        conv2mb = 1024*1024\n",
    "    except:\n",
    "        schema, _stats = u.get_type(\"opa\")  \n",
    "        tb, rb = schema[\"PortXmitData\"].index, schema[\"PortRcvData\"].index\n",
    "        conv2mb = 125000\n",
    "    for hostname, stats in _stats.items():\n",
    "        avg += stats[-1, tb] + stats[-1, rb] - \\\n",
    "               stats[0, tb] - stats[0, rb]\n",
    "    return avg/(u.dt*u.nhosts*conv2mb)\n",
    "\n",
    "def avg_flops(u):\n",
    "    schema, _stats = u.get_type(\"pmc\")\n",
    "    vector_widths = {\"SSE_D_ALL\" : 1, \"SIMD_D_256\" : 2, \n",
    "                \"FP_ARITH_INST_RETIRED_SCALAR_DOUBLE\" : 1, \n",
    "                 \"FP_ARITH_INST_RETIRED_128B_PACKED_DOUBLE\" : 2, \n",
    "                 \"FP_ARITH_INST_RETIRED_256B_PACKED_DOUBLE\" : 4, \n",
    "                 \"FP_ARITH_INST_RETIRED_512B_PACKED_DOUBLE\" : 8, \n",
    "                 \"SSE_DOUBLE_SCALAR\" : 1, \n",
    "                 \"SSE_DOUBLE_PACKED\" : 2, \n",
    "                 \"SIMD_DOUBLE_256\" : 4}\n",
    "    flops = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        for eventname in schema:\n",
    "            if eventname in vector_widths:\n",
    "                index = schema[eventname].index\n",
    "                flops += (stats[-1, index] - stats[0, index])*vector_widths[eventname]\n",
    "    return flops/(u.dt*u.nhosts*1e9)\n",
    "\n",
    "def avg_l1loadhits(u):\n",
    "    schema, _stats = u.get_type(\"pmc\")\n",
    "    load_names = ['LOAD_OPS_L1_HIT', 'MEM_UOPS_RETIRED_L1_HIT_LOADS']\n",
    "    loads = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        for eventname in schema:\n",
    "            if eventname in load_names:\n",
    "                index = schema[eventname].index\n",
    "                loads += stats[-1, index] - stats[0, index]\n",
    "    return loads/(u.dt*u.nhosts)\n",
    "\n",
    "def avg_l2loadhits(u):\n",
    "    schema, _stats = u.get_type(\"pmc\")\n",
    "    load_names = ['LOAD_OPS_L2_HIT', 'MEM_UOPS_RETIRED_L2_HIT_LOADS']\n",
    "    loads = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        for eventname in schema:\n",
    "            if eventname in load_names:\n",
    "                index = schema[eventname].index\n",
    "                loads += stats[-1, index] - stats[0, index]\n",
    "    return loads/(u.dt*u.nhosts)\n",
    "\n",
    "def avg_llcloadhits(u):\n",
    "    schema, _stats = u.get_type(\"pmc\")\n",
    "    load_names = ['LOAD_OPS_LLC_HIT', 'MEM_UOPS_RETIRED_LLC_HIT_LOADS']\n",
    "    loads = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        for eventname in schema:\n",
    "            if eventname in load_names:\n",
    "                index = schema[eventname].index\n",
    "                loads += stats[-1, index] - stats[0, index]\n",
    "    return loads/(u.dt*u.nhosts)\n",
    "\n",
    "def avg_lnetbw(u):\n",
    "    schema, _stats = u.get_type(\"lnet\")\n",
    "    bw = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        bw += stats[-1, schema[\"rx_bytes\"].index] + stats[-1, schema[\"tx_bytes\"].index] \\\n",
    "              - stats[0, schema[\"rx_bytes\"].index] - stats[0, schema[\"tx_bytes\"].index]\n",
    "    return bw/(1024*1024*u.dt*u.nhosts)\n",
    "\n",
    "def avg_lnetmsgs(u):\n",
    "    avg = 0\n",
    "    schema, _stats = u.get_type(\"lnet\")                  \n",
    "    tx, rx = schema[\"tx_msgs\"].index, schema[\"rx_msgs\"].index\n",
    "\n",
    "    for hostname, stats in _stats.items():\n",
    "        avg += stats[-1, tx] + stats[-1, rx] - \\\n",
    "               stats[0, tx] - stats[0, rx]\n",
    "    return avg/(u.dt*u.nhosts)\n",
    "\n",
    "def avg_loads(u):\n",
    "    schema, _stats = u.get_type(\"pmc\")\n",
    "    load_names = ['LOAD_OPS_ALL','MEM_UOPS_RETIRED_ALL_LOADS']\n",
    "    loads = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        for eventname in schema:\n",
    "            if eventname in load_names:\n",
    "                index = schema[eventname].index\n",
    "                loads += stats[-1, index] - stats[0, index]\n",
    "    return loads/(u.dt*u.nhosts)\n",
    "\n",
    "def avg_mbw(u):\n",
    "    schema, _stats = u.get_type(\"imc\")\n",
    "    avg = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        avg += stats[-1, schema[\"CAS_READS\"].index] + stats[-1, schema[\"CAS_WRITES\"].index] \\\n",
    "             - stats[0, schema[\"CAS_READS\"].index] - stats[0, schema[\"CAS_WRITES\"].index]\n",
    "    return 64.0*avg/(1024*1024*1024*u.dt*u.nhosts)\n",
    "\n",
    "def avg_mcdrambw(u):      \n",
    "    avg = 0\n",
    "    schema, _stats = u.get_type(\"intel_knl_edc_eclk\")\n",
    "    for hostname, stats in _stats.items():\n",
    "        avg += stats[-1, schema[\"RPQ_INSERTS\"].index] + stats[-1, schema[\"WPQ_INSERTS\"].index] \\\n",
    "             - stats[0, schema[\"RPQ_INSERTS\"].index] - stats[0, schema[\"WPQ_INSERTS\"].index]\n",
    "\n",
    "    if not \"flat\" in u.job.acct[\"queue\"].lower():\n",
    "        schema, _stats = u.get_type(\"intel_knl_edc_uclk\")\n",
    "        for hostname, stats in _stats.items():\n",
    "            avg -= stats[-1, schema[\"EDC_MISS_CLEAN\"].index] - stats[0, schema[\"EDC_MISS_CLEAN\"].index] + \\\n",
    "                stats[-1, schema[\"EDC_MISS_DIRTY\"].index] - stats[0, schema[\"EDC_MISS_DIRTY\"].index]\n",
    "\n",
    "        schema, _stats = u.get_type(\"intel_knl_mc_dclk\")\n",
    "        for hostname, stats in _stats.items():\n",
    "            avg -= stats[-1, schema[\"CAS_READS\"].index] - stats[0, schema[\"CAS_READS\"].index]\n",
    "\n",
    "    return 64.0*avg/(1024*1024*1024*u.dt*u.nhosts)\n",
    "\n",
    "def avg_mdcreqs(u):\n",
    "    schema, _stats = u.get_type(\"mdc\")\n",
    "    idx = schema[\"reqs\"].index\n",
    "    avg = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        avg += stats[-1, idx] - stats[0, idx]\n",
    "    return avg/(u.dt*u.nhosts)\n",
    "\n",
    "def avg_mdcwait(u):\n",
    "    schema, _stats = u.get_type(\"mdc\")\n",
    "    idx0, idx1 = schema[\"reqs\"].index, schema[\"wait\"].index\n",
    "    avg0, avg1 = 0, 0 \n",
    "    for hostname, stats in _stats.items():\n",
    "        avg0 += stats[-1, idx0] - stats[0, idx0]\n",
    "        avg1 += stats[-1, idx1] - stats[0, idx1]\n",
    "    return avg1/avg0\n",
    "\n",
    "def avg_openclose(u):\n",
    "    schema, _stats = u.get_type(\"llite\")\n",
    "    idx0, idx1 = schema[\"open\"].index, schema[\"close\"].index\n",
    "    avg = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        avg += stats[-1, idx0] - stats[0, idx0] + \\\n",
    "            stats[-1, idx1] - stats[0, idx1]\n",
    "    return avg/(u.dt*u.nhosts)\n",
    "\n",
    "def avg_oscreqs(u):\n",
    "    schema, _stats = u.get_type(\"osc\")\n",
    "    idx = schema[\"reqs\"].index\n",
    "    avg = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        avg += stats[-1, idx] - stats[0, idx]\n",
    "    return avg/(u.dt*u.nhosts)\n",
    "\n",
    "def avg_oscwait(u):\n",
    "    schema, _stats = u.get_type(\"osc\")\n",
    "    idx0, idx1 = schema[\"reqs\"].index, schema[\"wait\"].index\n",
    "    avg0, avg1 = 0, 0 \n",
    "    for hostname, stats in _stats.items():\n",
    "        avg0 += stats[-1, idx0] - stats[0, idx0]\n",
    "        avg1 += stats[-1, idx1] - stats[0, idx1]\n",
    "    return avg1/avg0\n",
    "\n",
    "def avg_packetsize(u):\n",
    "    try:\n",
    "        schema, _stats = u.get_type(\"ib_ext\")              \n",
    "        tx, rx = schema[\"port_xmit_pkts\"].index, schema[\"port_rcv_pkts\"].index\n",
    "        tb, rb = schema[\"port_xmit_data\"].index, schema[\"port_rcv_data\"].index\n",
    "        conv2mb = 1024*1024\n",
    "    except:\n",
    "        schema, _stats = u.get_type(\"opa\")  \n",
    "        tx, rx = schema[\"PortXmitPkts\"].index, schema[\"PortRcvPkts\"].index\n",
    "        tb, rb = schema[\"PortXmitData\"].index, schema[\"PortRcvData\"].index\n",
    "        conv2mb = 125000\n",
    "\n",
    "    npacks = 0\n",
    "    nbytes  = 0\n",
    "    for hostname, stats in _stats.items():\n",
    "        npacks += stats[-1, tx] + stats[-1, rx] - \\\n",
    "            stats[0, tx] - stats[0, rx]\n",
    "        nbytes += stats[-1, tb] + stats[-1, rb] - \\\n",
    "            stats[0, tb] - stats[0, rb]\n",
    "    return nbytes/(npacks*conv2mb)\n",
    "\n",
    "def max_fabricbw(u):\n",
    "    max_bw=0\n",
    "    try:\n",
    "        schema, _stats = u.get_type(\"ib_ext\")              \n",
    "        tx, rx = schema[\"port_xmit_data\"].index, schema[\"port_rcv_data\"].index\n",
    "        conv2mb = 1024*1024\n",
    "    except:\n",
    "        schema, _stats = u.get_type(\"opa\")  \n",
    "        tx, rx = schema[\"PortXmitData\"].index, schema[\"PortRcvData\"].index\n",
    "        conv2mb = 125000\n",
    "    for hostname, stats in _stats.items():\n",
    "        max_bw = max(max_bw, amax(diff(stats[:, tx] + stats[:, rx])/diff(u.t)))\n",
    "    return max_bw/conv2mb\n",
    "\n",
    "def max_lnetbw(u):\n",
    "    max_bw=0.0\n",
    "    schema, _stats = u.get_type(\"lnet\")              \n",
    "    tx, rx = schema[\"tx_bytes\"].index, schema[\"rx_bytes\"].index\n",
    "    for hostname, stats in _stats.items():\n",
    "        max_bw = max(max_bw, amax(diff(stats[:, tx] + stats[:, rx])/diff(u.t)))\n",
    "    return max_bw/(1024*1024)\n",
    "\n",
    "def max_mds(u):\n",
    "    max_mds = 0\n",
    "    schema, _stats = u.get_type(\"llite\")  \n",
    "    for hostname, stats in _stats.items():\n",
    "        max_mds = max(max_mds, amax(diff(stats[:, schema[\"open\"].index] + \\\n",
    "                                   stats[:, schema[\"close\"].index] + \\\n",
    "                                   stats[:, schema[\"mmap\"].index] + \\\n",
    "                                   stats[:, schema[\"fsync\"].index] + \\\n",
    "                                   stats[:, schema[\"setattr\"].index] + \\\n",
    "                                   stats[:, schema[\"truncate\"].index] + \\\n",
    "                                   stats[:, schema[\"flock\"].index] + \\\n",
    "                                   stats[:, schema[\"getattr\"].index] + \\\n",
    "                                   stats[:, schema[\"statfs\"].index] + \\\n",
    "                                   stats[:, schema[\"alloc_inode\"].index] + \\\n",
    "                                   stats[:, schema[\"setxattr\"].index] + \\\n",
    "                                   stats[:, schema[\"listxattr\"].index] + \\\n",
    "                                   stats[:, schema[\"removexattr\"].index] + \\\n",
    "                                   stats[:, schema[\"readdir\"].index] + \\\n",
    "                                   stats[:, schema[\"create\"].index] + \\\n",
    "                                   stats[:, schema[\"lookup\"].index] + \\\n",
    "                                   stats[:, schema[\"link\"].index] + \\\n",
    "                                   stats[:, schema[\"unlink\"].index] + \\\n",
    "                                   stats[:, schema[\"symlink\"].index] + \\\n",
    "                                   stats[:, schema[\"mkdir\"].index] + \\\n",
    "                                   stats[:, schema[\"rmdir\"].index] + \\\n",
    "                                   stats[:, schema[\"mknod\"].index] + \\\n",
    "                                   stats[:, schema[\"rename\"].index])/diff(u.t)))\n",
    "    return max_mds\n",
    "\n",
    "def max_packetrate(u):\n",
    "    max_pr=0\n",
    "    try:\n",
    "        schema, _stats = u.get_type(\"ib_ext\")              \n",
    "        tx, rx = schema[\"port_xmit_pkts\"].index, schema[\"port_rcv_pkts\"].index\n",
    "    except:\n",
    "        schema, _stats = u.get_type(\"opa\")  \n",
    "        tx, rx = schema[\"PortXmitPkts\"].index, schema[\"PortRcvPkts\"].index\n",
    "\n",
    "    for hostname, stats in _stats.items():\n",
    "        max_pr = max(max_pr, amax(diff(stats[:, tx] + stats[:, rx])/diff(u.t)))\n",
    "    return max_pr\n",
    "\n",
    "# This will compute the maximum memory usage recorded\n",
    "# by monitor.  It only samples at x mn intervals and\n",
    "# may miss high water marks in between.   \n",
    "def mem_hwm(u):\n",
    "    # mem usage in GB\n",
    "    max_memusage = 0.0 \n",
    "    schema, _stats = u.get_type(\"mem\")\n",
    "    for hostname, stats in _stats.items():\n",
    "        max_memusage = max(max_memusage, amax(stats[:, schema[\"MemUsed\"].index] - \\\n",
    "                          stats[:, schema[\"Slab\"].index] - \\\n",
    "                          stats[:, schema[\"FilePages\"].index]))\n",
    "    return max_memusage/(2.**30)\n",
    "\n",
    "def node_imbalance(u):\n",
    "    schema, _stats = u.get_type(\"cpu\")\n",
    "    max_usage = zeros(u.nt - 1)\n",
    "    for hostname, stats in _stats.items():\n",
    "        max_usage = maximum(max_usage, diff(stats[:, schema[\"user\"].index])/diff(u.t))\n",
    "\n",
    "    max_imbalance = []\n",
    "    for hostname, stats in _stats.items():\n",
    "        max_imbalance += [mean((max_usage - diff(stats[:, schema[\"user\"].index])/diff(u.t))/max_usage)]    \n",
    "    return amax([0. if isnan(x) else x for x in max_imbalance])\n",
    "\n",
    "def time_imbalance(u):\n",
    "    tmid=(u.t[:-1] + u.t[1:])/2.0\n",
    "    dt = diff(u.t)\n",
    "    schema, _stats = u.get_type(\"cpu\")    \n",
    "    vals = []\n",
    "    for hostname, stats in _stats.items():\n",
    "        #skip first and last two time slices\n",
    "        for i in [x + 2 for x in range(len(u.t) - 4)]:\n",
    "            r1=range(i)\n",
    "            r2=[x + i for x in range(len(dt) - i)]\n",
    "            rate = diff(stats[:, schema[\"user\"].index])/diff(u.t)\n",
    "            # integral before time slice \n",
    "            a = trapz(rate[r1], tmid[r1])/(tmid[i] - tmid[0])\n",
    "            # integral after time slice\n",
    "            b = trapz(rate[r2], tmid[r2])/(tmid[-1] - tmid[i])\n",
    "            # ratio of integral after time over before time\n",
    "            vals += [b/a]        \n",
    "    if vals:\n",
    "        return min(vals)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def avg_sf_evictrate(u):\n",
    "    schema, _stats = u.get_type(\"cha\")\n",
    "    sf_evictions = 0\n",
    "    llc_lookup = 0                  \n",
    "    for hostname, stats in _stats.items():\n",
    "        sf_evictions += stats[-1, schema[\"SF_EVICTIONS_MES\"].index] - \\\n",
    "                  stats[0, schema[\"SF_EVICTIONS_MES\"].index]\n",
    "        llc_lookup   += stats[-1, schema[\"LLC_LOOKUP_DATA_READ_LOCAL\"].index] - \\\n",
    "                  stats[0, schema[\"LLC_LOOKUP_DATA_READ_LOCAL\"].index] \n",
    "    return sf_evictions/llc_lookup\n",
    "\n",
    "def avg_page_hitrate(u):\n",
    "    schema, _stats = u.get_type(\"imc\")\n",
    "    act = 0\n",
    "    cas = 0                  \n",
    "    for hostname, stats in _stats.items():\n",
    "        act += stats[-1, schema[\"ACT_COUNT\"].index] - \\\n",
    "             stats[0, schema[\"ACT_COUNT\"].index]\n",
    "        cas += stats[-1, schema[\"CAS_READS\"].index] + stats[-1, schema[\"CAS_WRITES\"].index] - \\\n",
    "             stats[0, schema[\"CAS_READS\"].index] - stats[0, schema[\"CAS_WRITES\"].index]\n",
    "    return (cas - act) / cas\n",
    "\n",
    "def max_sf_evictrate(u):\n",
    "    schema, _stats = u.get_type(\"cha\", aggregate = False)\n",
    "    max_rate = 0\n",
    "    for hostname, dev in _stats.items():    \n",
    "        sf_evictions = {}\n",
    "        llc_lookup = {}\n",
    "        \n",
    "        for devname, stats in dev.items():\n",
    "            socket = devname.split('/')[0]\n",
    "            sf_evictions.setdefault(socket, 0)\n",
    "            sf_evictions[socket] += stats[-1, schema[\"SF_EVICTIONS_MES\"].index] - \\\n",
    "                                    stats[0, schema[\"SF_EVICTIONS_MES\"].index]\n",
    "            llc_lookup.setdefault(socket, 0)\n",
    "            llc_lookup[socket]   += stats[-1, schema[\"LLC_LOOKUP_DATA_READ_LOCAL\"].index] - \\\n",
    "                                    stats[0, schema[\"LLC_LOOKUP_DATA_READ_LOCAL\"].index]\n",
    "\n",
    "    for socket in set([x.split('/')[0] for x in dev.keys()]):\n",
    "        max_rate = max(sf_evictions[socket]/llc_lookup[socket], max_rate)\n",
    "    return max_rate\n",
    "\n",
    "def max_load15(u):\n",
    "    max_load15 = 0.0 \n",
    "    schema, _stats = u.get_type(\"ps\")\n",
    "    for hostname, stats in _stats.items():\n",
    "        max_load15 = max(max_load15, amax(stats[:, schema[\"load_15\"].index]))\n",
    "    return max_load15/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "utils instance has no attribute 'cha'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-d653facaa5eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"avg_l2loadhits [#/s]\"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mavg_l2loadhits\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mu_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"avg_llcloadhits [#/s]\"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mavg_llcloadhits\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mu_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;34m\"avg_sf_evictrate [#evicts/#rds]\"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mavg_sf_evictrate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mu_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"max_sf_evictrate [#evicts/#rds]\"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mmax_sf_evictrate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mu_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m\"avg_mbw [GB/s]\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_mbw\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mu_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-093977cfcda2>\u001b[0m in \u001b[0;36mavg_sf_evictrate\u001b[0;34m(u)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mavg_sf_evictrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cha\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[0msf_evictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0mllc_lookup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-a78aea20140f>\u001b[0m in \u001b[0;36mget_type\u001b[0;34m(self, typename, aggregate)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtypename\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"imc\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtypename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtypename\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pmc\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtypename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtypename\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cha\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtypename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtypename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: utils instance has no attribute 'cha'"
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"avg_cpuusage [#cores]\": avg_cpuusage( u_list[0] ),\n",
    "    \"mem_hwm [GB]\": mem_hwm ( u_list[0] ),\n",
    "    \"node_imbalance\":  node_imbalance( u_list[0] ),\n",
    "    \"time_imbalance\":  time_imbalance( u_list[0] ),\n",
    "    \"avg_flops [GF]\":  avg_flops( u_list[0] ),\n",
    "    \"avg_cpi [cyc/ins]\":  avg_cpi( u_list[0] ),\n",
    "    \"avg_freq [GHz]\":  avg_freq( u_list[0] ),\n",
    "    \"avg_loads [#/s]\":  avg_loads ( u_list[0] ),\n",
    "    \"avg_l1loadhits [#/s]\":  avg_l1loadhits( u_list[0] ),\n",
    "    \"avg_l2loadhits [#/s]\":  avg_l2loadhits( u_list[0] ),\n",
    "    \"avg_llcloadhits [#/s]\":  avg_llcloadhits ( u_list[0] ),\n",
    "    \"avg_sf_evictrate [#evicts/#rds]\":  avg_sf_evictrate( u_list[0] ),\n",
    "    \"max_sf_evictrate [#evicts/#rds]\":  max_sf_evictrate( u_list[0] ),\n",
    "    \"avg_mbw [GB/s]\": avg_mbw( u_list[0] ),\n",
    "    \"avg_page_hitrate [hits/cas]\": avg_page_hitrate( u_list[0] ),\n",
    "    \"avg_mcdrambw [GB/s]\": avg_mcdrambw( u_list[0] ),\n",
    "    \"avg_fabricbw [MB/s]\": avg_fabricbw( u_list[0] ),\n",
    "    \"max_fabricbw [MB/s]\": max_fabricbw( u_list[0] ),\n",
    "    \"avg_packetsize [MB]\": avg_packetsize( u_list[0] ),\n",
    "    \"max_packetrate [#/s]\": max_packetrate( u_list[0] ),\n",
    "    \"avg_ethbw [MB/s]\":  avg_ethbw  ( u_list[0] ),\n",
    "    \"max_mds [#/s]\" : max_mds( u_list[0] ),\n",
    "    \"avg_lnetmsgs [#/s]\": avg_lnetmsgs( u_list[0] ),\n",
    "    \"avg_lnetbw [MB/s]\":  avg_lnetbw( u_list[0] ),\n",
    "    \"max_lnetbw [MB/s]\":  max_lnetbw( u_list[0] ),\n",
    "    \"avg_mdcreqs [#/s]\":  avg_mdcreqs( u_list[0] ),\n",
    "    \"avg_mdcwait [us]\":  avg_mdcwait( u_list[0] ),\n",
    "    \"avg_oscreqs [#/s]\":  avg_oscreqs( u_list[0] ),\n",
    "    \"avg_oscwait [us]\":  avg_oscwait( u_list[0] ),\n",
    "    \"avg_openclose [#/s]\": avg_openclose( u_list[0] ),\n",
    "    \"avg_blockbw [MB/s]\": avg_blockbw( u_list[0] ),\n",
    "    \"max_load15 [cores]\": max_load15( u_list[0] )\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
