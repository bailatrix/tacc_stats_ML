{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File dependencies\n",
    "from tacc_stats.pickler.job_stats import Job\n",
    "import cPickle as pickle\n",
    "import argparse\n",
    "import time as clock\n",
    "from os import listdir\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of all pickled jobs via comet\n",
    "source_dir = '/oasis/projects/nsf/sys200/tcooper/xsede_stats/comet_pickles/'\n",
    "\n",
    "# List of date directories in source_dir\n",
    "dates_list = [ date for date in listdir(source_dir) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and open pickled job files\n",
    "**Process:**\n",
    "    - Iterate through the non-empty date folders available in source_dir\n",
    "    - A file is saved in valid_jobs if:\n",
    "        * The pickled file is a Job object\n",
    "        * The job ran for more than 6 cycles (1 hour)\n",
    "        * The total number of jobs saved at the end of the previous date folder is less than 1000\n",
    "            _This is purely to keep the computations manageable according to compute time requested_\n",
    "    - Exceptions are skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = float(sum([len(listdir(source_dir+date)) for date in dates_list]))\n",
    "valid_jobs = []\n",
    "t0 = clock.time()\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in dates_list:\n",
    "    \n",
    "    # do not open empty folders\n",
    "    if len( listdir(source_dir+date) ) != 0 and len(valid_jobs) < 1000:\n",
    "        size = len(listdir(source_dir+date))\n",
    "        \n",
    "        for job in listdir(source_dir+date):\n",
    "            count += 1\n",
    "            clear_output(wait=True)\n",
    "            print(\"Processing file {} of {} files for {} \\t ({}% of total)\"\n",
    "                  .format(count, size, date, np.round( count/n*100, 2)))\n",
    "            \n",
    "            # open job file if possible\n",
    "            try:\n",
    "                pickle_file = open( source_dir+date+'/'+job, 'rb')\n",
    "                jobid = pickle.load(pickle_file)\n",
    "                \n",
    "                # only save jobs that ran longer than 1 hour\n",
    "                if (len(jobid.times) > 5):\n",
    "                    valid_jobs.append(jobid)\n",
    "                    \n",
    "                pickle_file.close()\n",
    "                \n",
    "            except:\n",
    "                next \n",
    "            t2 = clock.time()\n",
    "            print(\"total: {}s\".format(np.round(t2-t0, 1)))\n",
    "    else:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of jobs accessed\n",
    "len(valid_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Columns & Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Given DataFrame, rename all columns with full label\n",
    "<b>Notes:</b>\n",
    "    - Certain column descriptions are repeated exactly in the available documentation. As a result, when these columns are relabelled according to their description, a column-specific identifier is appended in parentheses to keep it unique and prevent altering the meaning unintentionally.\n",
    "    - Some of the intel categories are listed in the available documention as \"-snb(hsw)-\"; however, the code is actually tagged with \"-hsw-\". This is to note the respective categories are in fact included in this program, though they appear skipped.\n",
    "    - At least one stat type was present in the data but does not appear to have a corresponding value in the available documentation, 'intel_rapl'. This has been interpretted to represent, \"Running Average Power Limit\" and is included in the proceeding analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatTitles ( df ):\n",
    "    return df.rename( columns={\"index\": \"Job Name\",\n",
    "              \"amd64_pmc\": \"AMD Opteron performance counters (per core)\",\n",
    "              \"intel_hsw\": \"Intel Haswell Processor (HSW) (per core)\",\n",
    "              \"intel_hsw_ht\": \"Intel Haswell Processor - Hyper-threaded (per logical core)\",\n",
    "              \"intel_nhm\": \"Intel Nehalem Processor (NHM) (per core)\",\n",
    "              \"intel_uncore\": \"Westmere Uncore (WTM) (per socket)\",\n",
    "              \"intel_snb\": \"Intel Sandy Brige (SNB) or Ivy Bridge (IVB) Processor (per core)\",\n",
    "              \"intel_rapl\": \"Running average power limit\",\n",
    "              \"intel_hsw_cbo\": \"Caching Agent (CBo) for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_pcu\": \"Power Control Unit for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_imc\": \"Integrated Memory Controller for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_qpi\": \"QPI Link Layer for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_hau\": \"Home Agent Unit for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_r2pci\": \"Ring to PCIe Agent for SNB (HSW) (per socket)\",\n",
    "              \"ib\": \"Infiniband usage (default)\",\n",
    "              \"ib_sw\": \"InfiniBand usage (sw)\",\n",
    "              \"ib_ext\": \"Infiniband usage (ext)\",\n",
    "              \"llite\": \"Lustre filesystem usage (per mount)\",\n",
    "              \"lnet\": \"Lustre network usage (lnet)\",\n",
    "              \"mdc\": \"Lustre network usage (mdc)\",\n",
    "              \"mic\": \"MIC scheduler account (per hardware thread)\",\n",
    "              \"osc\": \"Lustre filesystem usage (osc)\",\n",
    "              \"block\": \"Block device statistics (per device)\",\n",
    "              \"cpu\": \"Scheduler accounting (per CPU)\",\n",
    "              \"mem\": \"Memory usage (per socket)\",\n",
    "              \"net\": \"Network device usage (per device)\",\n",
    "              \"nfs\": \"NFS system usage\",\n",
    "              \"numa\": \"NUMA statistics (per socket)\",\n",
    "              \"proc\": \"Process specific data (MaxRSS, executable name etc.)\",\n",
    "              \"ps\": \"Process statistics\",\n",
    "              \"sysv_shm\": \"SysV shared memory segment usage\",\n",
    "              \"tmpfs\": \"Ram-backed filesystem usage (per mount)\",\n",
    "              \"vfs\": \"Dentry/file/inode cache usage\",\n",
    "              \"vm\": \"Virtual memory statistics\"\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = {}\n",
    "schemas_devices = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_jobs[0].schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops in loops in loops (Cleaning data)\n",
    "**Notes:**\n",
    "    - If a value is missing from the data, it will be replaced with '0' for the purpose of this project\n",
    "    - If a type of statistic was not collected on the job, that column is dropped from the DataFrame\n",
    "    - Two files are created during each iteration:\n",
    "         1) A .csv of the descriptive statistics for that host,job pair\n",
    "         2) A full .csv of the host,job data from the formatted DataFrame\n",
    "    - Naming convention: Files are labelled as '{host}_{jobid}' to support random lookup\n",
    "         * A job run on multiple host nodes is processed and saved with each individual host,job pair *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general = [\"Overall Average\", \"Standard Deviation\", \"High\", \"Low\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "n = float(len(valid_jobs))\n",
    "t0 = clock.time()\n",
    "\n",
    "for job in range( len(valid_jobs) ):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # general job values\n",
    "    jobid = valid_jobs[job]\n",
    "    start = pd.to_datetime(round(jobid.start_time), unit='s').time()\n",
    "    end = pd.to_datetime(round(jobid.end_time), unit='s').time()\n",
    "    numCycles = len(jobid.times)\n",
    "    total += 1\n",
    "    type_avgs = {}\n",
    "    times = []\n",
    "    \n",
    "    ##################################\n",
    "    #  build master list of schemas  #\n",
    "    ##################################\n",
    "    for stat in jobid.schemas.keys():\n",
    "        if stat not in schemas.keys():\n",
    "             schemas[stat] = jobid.schemas[stat].keys()\n",
    "    \n",
    "    # iterate through each host object job was run on\n",
    "    for host_name, host in jobid.hosts.iteritems():\n",
    "        try:\n",
    "            print(\"Processing hosts for job {} of {} \\t ({}% of total)\".format(job+1, int(n), np.round( (job+1)/n*100, 2)))\n",
    "            \n",
    "            ##################################\n",
    "            #    convert timestamps to dt    #\n",
    "            ##################################\n",
    "            times.append(start)\n",
    "            for time in host.times:\n",
    "                times.append( pd.to_datetime(round(time), unit='s').time() )\n",
    "            times.append(end)\n",
    "            \n",
    "            ##################################\n",
    "            #  build master list of devices  #\n",
    "            ##################################\n",
    "            for stat in host.stats.keys():\n",
    "                if stat not in schemas_devices.keys():\n",
    "                    schemas_devices[stat] = host.stats[stat].keys()\n",
    "                 \n",
    "            indices_all = []\n",
    "            for stat,devices in schemas_devices.items():\n",
    "                for device in devices:\n",
    "                    for schema in schemas[stat]:\n",
    "                        indices_all.append( (stat,device,schema) )\n",
    "    \n",
    "            all_idx = pd.MultiIndex.from_tuples(indices_all, names=['Stat', 'Device', 'Schema'])  \n",
    "            all_df = pd.DataFrame( index=all_idx, columns=times ).sort_index()\n",
    "            \n",
    "            ##################################\n",
    "            #   iterate through host.stats   #\n",
    "            ##################################\n",
    "            for host_name,host in jobid.hosts.items():\n",
    "                for stat,devices in host.stats.items():\n",
    "                    for device,cycles in devices.items():\n",
    "                        for i in range(len(cycles)):\n",
    "                            for j in range(len(cycles[i])):\n",
    "                                try:\n",
    "                                    time = times[i]\n",
    "                                    schema = schemas[stat][j]\n",
    "                                    all_df.loc[(stat,device,schema),time] = cycles[i][j]\n",
    "                                except:\n",
    "                                    next\n",
    "                        \n",
    "            #t2 = clock.time()\n",
    "            #print(\"total: {}s\".format(np.round(t2-t0, 1)))\n",
    "            \n",
    "            all_df.to_csv(path_or_buf=\"./jobs/all/{}_{}.csv\".format( host_name, jobid.id ))\n",
    "            \n",
    "        except:\n",
    "            next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that no job was missed\n",
    "if total == len(valid_jobs):\n",
    "    print \"Success!\"\n",
    "else:\n",
    "    print len(valid_jobs) - total, \"jobs missing\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
