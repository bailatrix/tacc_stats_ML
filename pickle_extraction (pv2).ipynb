{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File dependencies\n",
    "from tacc_stats.pickler.job_stats import Job\n",
    "import cPickle as pickle\n",
    "import argparse\n",
    "import time as clock\n",
    "from os import listdir\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of all pickled jobs via comet\n",
    "source_dir = '/oasis/projects/nsf/sys200/tcooper/xsede_stats/comet_pickles/'\n",
    "\n",
    "# List of date directories in source_dir\n",
    "dates_list = [ date for date in listdir(source_dir) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and open pickled job files\n",
    "**Process:**\n",
    "    - Iterate through the non-empty date folders available in source_dir\n",
    "    - A file is saved in valid_jobs if:\n",
    "        * The pickled file is a Job object\n",
    "        * The job ran for more than 6 cycles (1 hour)\n",
    "        * The total number of jobs saved at the end of the previous date folder is less than 1000\n",
    "            _This is purely to keep the computations manageable according to compute time requested_\n",
    "    - Exceptions are skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = float(sum([len(listdir(source_dir+date)) for date in dates_list]))\n",
    "valid_jobs = []\n",
    "t0 = clock.time()\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in dates_list:\n",
    "    \n",
    "    # do not open empty folders\n",
    "    if len( listdir(source_dir+date) ) != 0 and len(valid_jobs) < 1000:\n",
    "        size = len(listdir(source_dir+date))\n",
    "        \n",
    "        for job in listdir(source_dir+date):\n",
    "            count += 1\n",
    "            clear_output(wait=True)\n",
    "            print(\"Processing file {} of {} files for {} \\t ({}% of total)\"\n",
    "                  .format(count, size, date, np.round( count/n*100, 2)))\n",
    "            \n",
    "            # open job file if possible\n",
    "            try:\n",
    "                pickle_file = open( source_dir+date+'/'+job, 'rb')\n",
    "                jobid = pickle.load(pickle_file)\n",
    "                \n",
    "                # only save jobs that ran longer than 1 hour\n",
    "                if (len(jobid.times) > 5):\n",
    "                    valid_jobs.append(jobid)\n",
    "                    \n",
    "                pickle_file.close()\n",
    "                \n",
    "            except:\n",
    "                next \n",
    "            t2 = clock.time()\n",
    "            print(\"total: {}s\".format(np.round(t2-t0, 1)))\n",
    "    else:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of jobs accessed\n",
    "len(valid_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Columns & Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Given DataFrame, rename all columns with full label\n",
    "<b>Notes:</b>\n",
    "    - Certain column descriptions are repeated exactly in the available documentation. As a result, when these columns are relabelled according to their description, a column-specific identifier is appended in parentheses to keep it unique and prevent altering the meaning unintentionally.\n",
    "    - Some of the intel categories are listed in the available documention as \"-snb(hsw)-\"; however, the code is actually tagged with \"-hsw-\". This is to note the respective categories are in fact included in this program, though they appear skipped.\n",
    "    - At least one stat type was present in the data but does not appear to have a corresponding value in the available documentation, 'intel_rapl'. This has been interpretted to represent, \"Running Average Power Limit\" and is included in the proceeding analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Master list of all possible statistics collected\n",
    "stat_types = [\"amd64_pmc\", \"intel_hsw\", \"intel_hsw_ht\", \"intel_nhm\",\n",
    "              \"intel_uncore\", \"intel_snb\", \"intel_rapl\", \"intel_hsw_cbo\", \"intel_hsw_pcu\",\n",
    "              \"intel_hsw_imc\", \"intel_hsw_qpi\", \"intel_hsw_hau\",\n",
    "              \"intel_hsw_r2pci\", \"ib\", \"ib_sw\", \"ib_ext\", \"llite\",\n",
    "              \"lnet\", \"mdc\", \"mic\", \"osc\", \"block\", \"cpu\", \"mem\", \"net\",\n",
    "              \"nfs\", \"numa\", \"proc\", \"ps\", \"sysv_shm\", \"tmpfs\", \"vfs\", \"vm\"]\n",
    "\n",
    "# Total number of categories\n",
    "len(stat_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatTitles ( df ):\n",
    "    return df.rename( columns={\"index\": \"Job Name\",\n",
    "              \"amd64_pmc\": \"AMD Opteron performance counters (per core)\",\n",
    "              \"intel_hsw\": \"Intel Haswell Processor (HSW) (per core)\",\n",
    "              \"intel_hsw_ht\": \"Intel Haswell Processor - Hyper-threaded (per logical core)\",\n",
    "              \"intel_nhm\": \"Intel Nehalem Processor (NHM) (per core)\",\n",
    "              \"intel_uncore\": \"Westmere Uncore (WTM) (per socket)\",\n",
    "              \"intel_snb\": \"Intel Sandy Brige (SNB) or Ivy Bridge (IVB) Processor (per core)\",\n",
    "              \"intel_rapl\": \"Running average power limit\",\n",
    "              \"intel_hsw_cbo\": \"Caching Agent (CBo) for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_pcu\": \"Power Control Unit for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_imc\": \"Integrated Memory Controller for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_qpi\": \"QPI Link Layer for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_hau\": \"Home Agent Unit for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_r2pci\": \"Ring to PCIe Agent for SNB (HSW) (per socket)\",\n",
    "              \"ib\": \"Infiniband usage (default)\",\n",
    "              \"ib_sw\": \"InfiniBand usage (sw)\",\n",
    "              \"ib_ext\": \"Infiniband usage (ext)\",\n",
    "              \"llite\": \"Lustre filesystem usage (per mount)\",\n",
    "              \"lnet\": \"Lustre network usage (lnet)\",\n",
    "              \"mdc\": \"Lustre network usage (mdc)\",\n",
    "              \"mic\": \"MIC scheduler account (per hardware thread)\",\n",
    "              \"osc\": \"Lustre filesystem usage (osc)\",\n",
    "              \"block\": \"Block device statistics (per device)\",\n",
    "              \"cpu\": \"Scheduler accounting (per CPU)\",\n",
    "              \"mem\": \"Memory usage (per socket)\",\n",
    "              \"net\": \"Network device usage (per device)\",\n",
    "              \"nfs\": \"NFS system usage\",\n",
    "              \"numa\": \"NUMA statistics (per socket)\",\n",
    "              \"proc\": \"Process specific data (MaxRSS, executable name etc.)\",\n",
    "              \"ps\": \"Process statistics\",\n",
    "              \"sysv_shm\": \"SysV shared memory segment usage\",\n",
    "              \"tmpfs\": \"Ram-backed filesystem usage (per mount)\",\n",
    "              \"vfs\": \"Dentry/file/inode cache usage\",\n",
    "              \"vm\": \"Virtual memory statistics\"\n",
    "                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops in loops in loops (Cleaning data)\n",
    "**Notes:**\n",
    "    - If a value is missing from the data, it will be replaced with '0' for the purpose of this project\n",
    "    - If a type of statistic was not collected on the job, that column is dropped from the DataFrame\n",
    "    - Two files are created during each iteration:\n",
    "         1) A .csv of the descriptive statistics for that host,job pair\n",
    "         2) A full .csv of the host,job data from the formatted DataFrame\n",
    "    - Naming convention: Files are labelled as '{host}_{jobid}' to support random lookup\n",
    "         * A job run on multiple host nodes is processed and saved with each individual host,job pair *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "general = [\"Overall Average\", \"Standard Deviation\", \"High\", \"Low\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hosts for job 72 of 73 \t (0% of total)\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "n = len(valid_jobs)\n",
    "t0 = clock.time()\n",
    "\n",
    "for job in range( len(valid_jobs) ):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # general job values\n",
    "    jobid = valid_jobs[job]\n",
    "    start = pd.to_datetime(round(jobid.start_time), unit='s').time()\n",
    "    end = pd.to_datetime(round(jobid.end_time), unit='s').time()\n",
    "    numCycles = len(jobid.times)\n",
    "    total += 1\n",
    "    type_avgs = {}\n",
    "    times = []\n",
    "    \n",
    "    \n",
    "    ##################################\n",
    "    #  create DataFrame for schemas  #\n",
    "    ##################################\n",
    "    sch_tups = []\n",
    "    \n",
    "    for stat,schemas in jobid.schemas.iteritems():\n",
    "        for schema in schemas.keys():\n",
    "            sch_tups.append((stat,schema))\n",
    "    \n",
    "    # iterate through each host object job was run on\n",
    "    for host_name, host in jobid.hosts.iteritems():\n",
    "        try:\n",
    "            devices = [ key for key in host.stats['intel_hsw'].keys() ]\n",
    "            print(\"Processing hosts for job {} of {} \\t ({}% of total)\".format(job, n, np.round( job/n*100, 2)))\n",
    "            \n",
    "            ##################################\n",
    "            #    convert timestamps to dt    #\n",
    "            ##################################\n",
    "            times.append(start)\n",
    "            for time in host.times:\n",
    "                times.append( pd.to_datetime(round(time), unit='s').time() )\n",
    "            times.append(end)\n",
    "            \n",
    "            ##################################\n",
    "            #   create DataFrame skeletons   #\n",
    "            ##################################\n",
    "            time_tups = [ (time,stat) for time in times for stat in general ]\n",
    "            sch_idx = pd.MultiIndex.from_tuples(sch_tups, names=['Stat', 'Schema'])\n",
    "            desc_idx = pd.MultiIndex.from_tuples(time_tups, names=['Stat', 'Schema'])\n",
    "            \n",
    "            schemas_df = pd.DataFrame( index=sch_idx, columns=times )\n",
    "            intel_hsw_df = pd.DataFrame( index=times, columns=devices )\n",
    "            desc_df = pd.DataFrame( index=desc_idx, columns=host.stats.keys() )\n",
    "            job_df = pd.DataFrame( index=times, columns=host.stats.keys() )\n",
    "            \n",
    "            ##################################\n",
    "            #   iterate through host.stats   #\n",
    "            ##################################\n",
    "            idx = 0\n",
    "            \n",
    "            for i in range( len(host.stats.keys())): \n",
    "                stat = host.stats.keys()[i]\n",
    "                dev = host.stats[stat]\n",
    "                \n",
    "                for dev,cycles in dev.iteritems():                 \n",
    "                    for j in range(len(cycles)):\n",
    "                        time = times[j]\n",
    "                        cycle = cycles[j]\n",
    "                        \n",
    "                        for k in range(len(cycle)):\n",
    "                            if idx == len(sch_tups):\n",
    "                                idx = 0\n",
    "                            \n",
    "                            loc = sch_tups[idx]\n",
    "                            schemas_df.at[loc,time] = cycle[k]\n",
    "                            idx += 1\n",
    "                            \n",
    "                        if stat == 'intel_hsw':\n",
    "                            intel_hsw_df.loc[[time],[dev]] = cycle[8]           \n",
    "                            \n",
    "                        desc_df.loc[(time,\"Overall Average\"),[stat] ]    = np.mean(cycle )\n",
    "                        desc_df.loc[(time,\"Standard Deviation\"),[stat] ] = np.std( cycle )\n",
    "                        desc_df.loc[(time,\"High\"),[stat] ]               = np.max( cycle )\n",
    "                        desc_df.loc[(time,\"Low\"),[stat] ]                = np.min( cycle )\n",
    "                        \n",
    "                        job_df.loc[[time],[stat]] = sum(cycle) \n",
    "                        \n",
    "            #t2 = clock.time()\n",
    "            #print(\"total: {}s\".format(np.round(t2-t0, 1)))\n",
    "            \n",
    "            ###################################\n",
    "            #    relable and format columns   #\n",
    "            ###################################\n",
    "            intel_hsw_df['Total'] = intel_hsw_df.sum(axis=1)\n",
    "            desc_df = formatTitles( desc_df ).sort_index(axis=1)\n",
    "            job_df = formatTitles( job_df ).T\n",
    "            job_df.index.name = \"Cycle\"\n",
    "            \n",
    "            ###################################\n",
    "            #   save DataFrames to csv files  #\n",
    "            ###################################\n",
    "            schemas_df.to_csv(path_or_buf=\"./jobs/by_schema/{}_{}.csv\".format( host_name, jobid.id ))\n",
    "            intel_hsw_df.to_csv(path_or_buf=\"./jobs/instructions_retired/{}_{}.csv\".format( host_name, jobid.id ))\n",
    "            desc_df.to_csv(path_or_buf=\"./jobs/descriptive_stats/{}_{}.csv\".format( host_name, jobid.id ))\n",
    "            job_df.to_csv(path_or_buf=\"./jobs/by_sum/{}_{}.csv\".format( host_name, jobid.id ))\n",
    "            \n",
    "        except:\n",
    "            next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# check that no job was missed\n",
    "if total == len(valid_jobs):\n",
    "    print \"Success!\"\n",
    "else:\n",
    "    print len(valid_jobs) - total, \"jobs missing\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
