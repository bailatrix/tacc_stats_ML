{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File dependencies\n",
    "from tacc_stats.pickler.job_stats import Job\n",
    "import cPickle as pickle\n",
    "import argparse\n",
    "import time\n",
    "from os import listdir\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of all pickled jobs via comet\n",
    "source_dir = '/oasis/projects/nsf/sys200/tcooper/xsede_stats/comet_pickles/'\n",
    "\n",
    "# List of date directories in source_dir\n",
    "dates_list = [ date for date in listdir(source_dir) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and open pickled job files\n",
    "**Process:**\n",
    "    - Iterate through the non-empty date folders available in source_dir\n",
    "    - A file is saved in valid_jobs if:\n",
    "        * The pickled file is a Job object\n",
    "        * The job ran for more than 6 cycles (1 hour)\n",
    "        * The total number of jobs saved at the end of the previous date folder is less than 1000\n",
    "            _This is purely to keep the computations manageable according to compute time requested_\n",
    "    - Exceptions are skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = sum([len(listdir(source_dir+date)) for date in dates_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_jobs = []\n",
    "t0 = time.time()\n",
    "\n",
    "for date in dates_list:\n",
    "    count = 0\n",
    "    \n",
    "    # do not open empty folders\n",
    "    if len( listdir(source_dir+date) ) != 0 and len(valid_jobs) < 10000:\n",
    "        files = listdir(source_dir+date)\n",
    "        \n",
    "        for i in range(len(files)):\n",
    "            count += i\n",
    "            job = files[i]\n",
    "            clear_output(wait=True)\n",
    "            t1 = time.time()\n",
    "            print(\"Processing file {} ({}%)\".format(i, np.round( 100*i//n, 2)))\n",
    "            \n",
    "            # open job file if possible\n",
    "            try:\n",
    "                pickle_file = open( source_dir+date+'/'+job, 'rb')\n",
    "                jobid = pickle.load(pickle_file)\n",
    "                \n",
    "                # only save jobs that ran longer than 1 hour\n",
    "                if (len(jobid.times) > 5):\n",
    "                    valid_jobs.append(jobid)\n",
    "                    \n",
    "                pickle_file.close()\n",
    "            except:\n",
    "                next\n",
    "                \n",
    "            # t2 = time.time()\n",
    "            # print(\" {}s (total: {}s)\".format(np.round(t2-t1, 2), np.round(t2-t0, 2)))\n",
    "    else:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of jobs accessed\n",
    "len(valid_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate example host,job pair\n",
    "ex_job = valid_jobs[0].id\n",
    "ex_job_host = valid_jobs[0].hosts.keys()[0]\n",
    "ex_stats = valid_jobs[0].schemas.keys()\n",
    "ex_job_host, ex_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"There were\", len(ex_stats), \"statistics collected for this example job:\"\n",
    "print\n",
    "print [stat for stat in ex_stats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Columns & Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Given DataFrame, rename all columns with full label\n",
    "<b>Notes:</b>\n",
    "    - Certain column descriptions are repeated exactly in the available documentation. As a result, when these columns are relabelled according to their description, a column-specific identifier is appended in parentheses to keep it unique and prevent altering the meaning unintentionally.\n",
    "    - Some of the intel categories are listed in the available documention as \"-snb(hsw)-\"; however, the code is actually tagged with \"-hsw-\". This is to note the respective categories are in fact included in this program, though they appear skipped.\n",
    "    - At least one stat type was present in the data but does not appear to have a corresponding value in the available documentation, 'intel_rapl'. This has been interpretted to represent, \"Running Average Power Limit\" and is included in the proceeding analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master list of all possible statistics collected\n",
    "stat_types = [\"amd64_pmc\", \"intel_hsw\", \"intel_hsw_ht\", \"intel_nhm\",\n",
    "              \"intel_uncore\", \"intel_snb\", \"intel_rapl\", \"intel_hsw_cbo\", \"intel_hsw_pcu\",\n",
    "              \"intel_hsw_imc\", \"intel_hsw_qpi\", \"intel_hsw_hau\",\n",
    "              \"intel_hsw_r2pci\", \"ib\", \"ib_sw\", \"ib_ext\", \"llite\",\n",
    "              \"lnet\", \"mdc\", \"mic\", \"osc\", \"block\", \"cpu\", \"mem\", \"net\",\n",
    "              \"nfs\", \"numa\", \"proc\", \"ps\", \"sysv_shm\", \"tmpfs\", \"vfs\", \"vm\"]\n",
    "\n",
    "# Total number of categories\n",
    "len(stat_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatTitles ( df ):\n",
    "    return df.rename( columns={\"index\": \"Job Name\",\n",
    "              \"amd64_pmc\": \"AMD Opteron performance counters (per core)\",\n",
    "              \"intel_hsw\": \"Intel Haswell Processor (HSW) (per core)\",\n",
    "              \"intel_hsw_ht\": \"Intel Haswell Processor - Hyper-threaded (per logical core)\",\n",
    "              \"intel_nhm\": \"Intel Nehalem Processor (NHM) (per core)\",\n",
    "              \"intel_uncore\": \"Westmere Uncore (WTM) (per socket)\",\n",
    "              \"intel_snb\": \"Intel Sandy Brige (SNB) or Ivy Bridge (IVB) Processor (per core)\",\n",
    "              \"intel_rapl\": \"Running average power limit\",\n",
    "              \"intel_hsw_cbo\": \"Caching Agent (CBo) for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_pcu\": \"Power Control Unit for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_imc\": \"Integrated Memory Controller for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_qpi\": \"QPI Link Layer for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_hau\": \"Home Agent Unit for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_r2pci\": \"Ring to PCIe Agent for SNB (HSW) (per socket)\",\n",
    "              \"ib\": \"Infiniband usage (default)\",\n",
    "              \"ib_sw\": \"InfiniBand usage (sw)\",\n",
    "              \"ib_ext\": \"Infiniband usage (ext)\",\n",
    "              \"llite\": \"Lustre filesystem usage (per mount)\",\n",
    "              \"lnet\": \"Lustre network usage (lnet)\",\n",
    "              \"mdc\": \"Lustre network usage (mdc)\",\n",
    "              \"mic\": \"MIC scheduler account (per hardware thread)\",\n",
    "              \"osc\": \"Lustre filesystem usage (osc)\",\n",
    "              \"block\": \"Block device statistics (per device)\",\n",
    "              \"cpu\": \"Scheduler accounting (per CPU)\",\n",
    "              \"mem\": \"Memory usage (per socket)\",\n",
    "              \"net\": \"Network device usage (per device)\",\n",
    "              \"nfs\": \"NFS system usage\",\n",
    "              \"numa\": \"NUMA statistics (per socket)\",\n",
    "              \"proc\": \"Process specific data (MaxRSS, executable name etc.)\",\n",
    "              \"ps\": \"Process statistics\",\n",
    "              \"sysv_shm\": \"SysV shared memory segment usage\",\n",
    "              \"tmpfs\": \"Ram-backed filesystem usage (per mount)\",\n",
    "              \"vfs\": \"Dentry/file/inode cache usage\",\n",
    "              \"vm\": \"Virtual memory statistics\"\n",
    "                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops in loops in loops (Cleaning data)\n",
    "**Notes:**\n",
    "    - If a value is missing from the data, it will be replaced with '0' for the purpose of this project\n",
    "    - If a type of statistic was not collected on the job, that column is dropped from the DataFrame\n",
    "    - Two files are created during each iteration:\n",
    "         1) A .csv of the descriptive statistics for that host,job pair\n",
    "         2) A full .csv of the host,job data from the formatted DataFrame\n",
    "    - Naming convention: Files are labelled as '{host}_{jobid}' to support random lookup\n",
    "         * A job run on multiple host nodes is processed and saved with each individual host,job pair *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "\n",
    "for job in range( len(valid_jobs) ):\n",
    "\n",
    "    # general job values\n",
    "    jobid = valid_jobs[job]\n",
    "    start = pd.to_datetime(round(jobid.start_time), unit='s').time()\n",
    "    end = pd.to_datetime(round(jobid.end_time), unit='s').time()\n",
    "    numCycles = len(jobid.times) + 2\n",
    "    total += 1\n",
    "    type_avgs = {}\n",
    "    times = []\n",
    "    \n",
    "    # check that stat types all exist in master list\n",
    "    # append if not found\n",
    "    for stat in jobid.schemas.keys():\n",
    "        if stat not in stat_types:\n",
    "            stat_types.append(stat)\n",
    "    \n",
    "    # iterate through each host object job was run on\n",
    "    for host_name, host in jobid.hosts.iteritems():\n",
    "        typ='all'\n",
    "        dev='all'\n",
    "        \n",
    "        if 'all' in typ: typ_keys = host.stats.keys()\n",
    "        else: typ_keys = typ\n",
    "        \n",
    "        # convert timestamps to DateTime objects\n",
    "        times = [ pd.to_datetime(round(time), unit='s').time() for time in host.times ]\n",
    "        \n",
    "        # iterate through all types of stats\n",
    "        for type_name in typ_keys:\n",
    "            type_data = host.stats[type_name]\n",
    "            type_avgs[type_name] = []\n",
    "            \n",
    "            if 'all' in dev: dev_keys = type_data.keys()\n",
    "            else: dev_keys = dev\n",
    "            \n",
    "            # iterate through all devices each stat was collected on\n",
    "            # compute the mean of the row for each row in matrix\n",
    "            for dev_name in dev_keys:          \n",
    "                type_avgs[type_name].append(\n",
    "                    [  np.mean(row).round(decimals=3) for row in type_data[dev_name] ])\n",
    "        \n",
    "        # skeleton for job DataFrame\n",
    "        all_stats = { time:{ stat:0 for stat in type_avgs } for time in times }\n",
    "        \n",
    "        # skeleton for descriptive stats DataFrame\n",
    "        dev_stats = { type_stat:{} for type_stat in jobid.schemas.keys() }\n",
    "        \n",
    "        # parse through all values in matrices\n",
    "        # assign row,col pair in matrix to row,col pair in df\n",
    "        for type_avg,data in type_avgs.iteritems():\n",
    "            \n",
    "            dev_stats[type_avg][\"Overall Average\"] = np.mean(data)\n",
    "            dev_stats[type_avg][\"Standard Deviation\"] = np.std(data)\n",
    "            dev_stats[type_avg][\"High\"] = np.max(data)\n",
    "            dev_stats[type_avg][\"Low\"] = np.min(data)\n",
    "            \n",
    "            for dev in range( len(data) ):\n",
    "                for i in range( len(times)):\n",
    "                    try:\n",
    "                        all_stats[times[i]][type_avg] = data[dev][i].round(3)\n",
    "                        \n",
    "                    except:\n",
    "                        next\n",
    "        \n",
    "        # clean up descriptive stats and save to csv\n",
    "        desc_df = (formatTitles( pd.DataFrame( dev_stats )).dropna(axis=1)).set_index('Unnamed: 0')\n",
    "        desc_df.to_csv(path_or_buf=\"./jobs/descriptive_stats/{host}_{jobid}.csv\"\n",
    "                       .format( jobid=valid_jobs[job].id, host=host_name ))\n",
    "        \n",
    "        # create df\n",
    "        df = pd.DataFrame(all_stats)\n",
    "        \n",
    "        # transpose df so index=time, col=category of stat\n",
    "        # relable columns to formal title\n",
    "        df = formatTitles( df.T )\n",
    "        df.index.name = \"Cycle\"\n",
    "        \n",
    "        # save jobs to csv\n",
    "        # set some aside for testing model\n",
    "        if total <= (len(valid_jobs)*3/4):\n",
    "            df.to_csv(path_or_buf=\"./jobs/train/{host}_{jobid}.csv\"\n",
    "                           .format( jobid=valid_jobs[job].id, host=host_name ))\n",
    "        else:\n",
    "            df.to_csv(path_or_buf=\"./jobs/test/{host}_{jobid}.csv\"\n",
    "                           .format( jobid=valid_jobs[job].id, host=host_name ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that no job was missed\n",
    "if total == len(valid_jobs):\n",
    "    print \"Success!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example read of descriptive .csv file saved\n",
    "ex_desc_df = pd.DataFrame( pd.read_csv(\"./jobs/descriptive_stats/{host_A}_{job_A}.csv\"\n",
    "                          .format( host_A=ex_job_host, job_A=ex_job)) )\n",
    "ex_desc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example read of full .csv file saved\n",
    "ex_df = pd.DataFrame( pd.read_csv(\"./jobs/train/{host_A}_{job_A}.csv\"\n",
    "                          .format( host_A=ex_job_host, job_A=ex_job)) )\n",
    "ex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stat_types)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
