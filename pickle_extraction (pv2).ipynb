{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File dependencies\n",
    "from tacc_stats.pickler.job_stats import Job\n",
    "import cPickle as pickle\n",
    "import argparse\n",
    "import time\n",
    "from os import listdir\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of all pickled jobs via comet\n",
    "source_dir = '/oasis/projects/nsf/sys200/tcooper/xsede_stats/comet_pickles/'\n",
    "\n",
    "# List of date directories in source_dir\n",
    "dates_list = [ date for date in listdir(source_dir) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and open pickled job files\n",
    "**Process:**\n",
    "    - Iterate through the non-empty date folders available in source_dir\n",
    "    - A file is saved in valid_jobs if:\n",
    "        * The pickled file is a Job object\n",
    "        * The job ran for more than 6 cycles (1 hour)\n",
    "        * The total number of jobs saved at the end of the previous date folder is less than 1000\n",
    "            _This is purely to keep the computations manageable according to compute time requested_\n",
    "    - Exceptions are skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = float(sum([len(listdir(source_dir+date)) for date in dates_list]))\n",
    "valid_jobs = []\n",
    "t0 = time.time()\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 4559 of 25116 files for 2016-10-07 \t\t (0.21% of total)\n",
      "total: 55.6s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c9e05413ab02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             print(\"Processing file {} of {} files for {} \\t\\t ({}% of total)\"\n\u001b[1;32m     11\u001b[0m                   .format(count, size, date, np.round( count/n*100, 2)))\n",
      "\u001b[0;32m/home/baileyp/.local/lib/python2.7/site-packages/IPython/core/display.pyc\u001b[0m in \u001b[0;36mclear_output\u001b[0;34m(wait)\u001b[0m\n\u001b[1;32m   1224\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteractiveshell\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInteractiveShell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mInteractiveShell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m         \u001b[0mInteractiveShell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_pub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\033[2K\\r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/baileyp/.local/lib/python2.7/site-packages/ipykernel/zmqshell.pyc\u001b[0m in \u001b[0;36mclear_output\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m    156\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         self.session.send(\n\u001b[1;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'clear_output'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/baileyp/.local/lib/python2.7/site-packages/ipykernel/zmqshell.pyc\u001b[0m in \u001b[0;36m_flush_streams\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;34m\"\"\"flush IO Streams prior to display\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_thread_local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/baileyp/.local/lib/python2.7/site-packages/ipykernel/iostream.pyc\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0m_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgotit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for date in dates_list:\n",
    "    \n",
    "    # do not open empty folders\n",
    "    if len( listdir(source_dir+date) ) != 0 and len(valid_jobs) < 1:\n",
    "        size = len(listdir(source_dir+date))\n",
    "        \n",
    "        for job in listdir(source_dir+date):\n",
    "            count += 1\n",
    "            clear_output(wait=True)\n",
    "            print(\"Processing file {} of {} files for {} \\t ({}% of total)\"\n",
    "                  .format(count, size, date, np.round( count/n*100, 2)))\n",
    "            \n",
    "            # open job file if possible\n",
    "            try:\n",
    "                pickle_file = open( source_dir+date+'/'+job, 'rb')\n",
    "                jobid = pickle.load(pickle_file)\n",
    "                \n",
    "                # only save jobs that ran longer than 1 hour\n",
    "                if (len(jobid.times) > 5):\n",
    "                    valid_jobs.append(jobid)\n",
    "                    \n",
    "                pickle_file.close()\n",
    "                \n",
    "            except:\n",
    "                next \n",
    "            t2 = time.time()\n",
    "            print(\"total: {}s\".format(np.round(t2-t0, 1)))\n",
    "    else:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of jobs accessed\n",
    "len(valid_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate example host,job pair\n",
    "ex_job = valid_jobs[0].id\n",
    "ex_job_host = valid_jobs[0].hosts.keys()[0]\n",
    "ex_stats = valid_jobs[0].schemas.keys()\n",
    "ex_job_host, ex_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"There were\", len(ex_stats), \"statistics collected for this example job:\"\n",
    "print\n",
    "print [stat for stat in ex_stats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Columns & Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Given DataFrame, rename all columns with full label\n",
    "<b>Notes:</b>\n",
    "    - Certain column descriptions are repeated exactly in the available documentation. As a result, when these columns are relabelled according to their description, a column-specific identifier is appended in parentheses to keep it unique and prevent altering the meaning unintentionally.\n",
    "    - Some of the intel categories are listed in the available documention as \"-snb(hsw)-\"; however, the code is actually tagged with \"-hsw-\". This is to note the respective categories are in fact included in this program, though they appear skipped.\n",
    "    - At least one stat type was present in the data but does not appear to have a corresponding value in the available documentation, 'intel_rapl'. This has been interpretted to represent, \"Running Average Power Limit\" and is included in the proceeding analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master list of all possible statistics collected\n",
    "stat_types = [\"amd64_pmc\", \"intel_hsw\", \"intel_hsw_ht\", \"intel_nhm\",\n",
    "              \"intel_uncore\", \"intel_snb\", \"intel_rapl\", \"intel_hsw_cbo\", \"intel_hsw_pcu\",\n",
    "              \"intel_hsw_imc\", \"intel_hsw_qpi\", \"intel_hsw_hau\",\n",
    "              \"intel_hsw_r2pci\", \"ib\", \"ib_sw\", \"ib_ext\", \"llite\",\n",
    "              \"lnet\", \"mdc\", \"mic\", \"osc\", \"block\", \"cpu\", \"mem\", \"net\",\n",
    "              \"nfs\", \"numa\", \"proc\", \"ps\", \"sysv_shm\", \"tmpfs\", \"vfs\", \"vm\"]\n",
    "\n",
    "# Total number of categories\n",
    "len(stat_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatTitles ( df ):\n",
    "    return df.rename( columns={\"index\": \"Job Name\",\n",
    "              \"amd64_pmc\": \"AMD Opteron performance counters (per core)\",\n",
    "              \"intel_hsw\": \"Intel Haswell Processor (HSW) (per core)\",\n",
    "              \"intel_hsw_ht\": \"Intel Haswell Processor - Hyper-threaded (per logical core)\",\n",
    "              \"intel_nhm\": \"Intel Nehalem Processor (NHM) (per core)\",\n",
    "              \"intel_uncore\": \"Westmere Uncore (WTM) (per socket)\",\n",
    "              \"intel_snb\": \"Intel Sandy Brige (SNB) or Ivy Bridge (IVB) Processor (per core)\",\n",
    "              \"intel_rapl\": \"Running average power limit\",\n",
    "              \"intel_hsw_cbo\": \"Caching Agent (CBo) for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_pcu\": \"Power Control Unit for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_imc\": \"Integrated Memory Controller for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_qpi\": \"QPI Link Layer for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_hau\": \"Home Agent Unit for SNB (HSW) (per socket)\",\n",
    "              \"intel_hsw_r2pci\": \"Ring to PCIe Agent for SNB (HSW) (per socket)\",\n",
    "              \"ib\": \"Infiniband usage (default)\",\n",
    "              \"ib_sw\": \"InfiniBand usage (sw)\",\n",
    "              \"ib_ext\": \"Infiniband usage (ext)\",\n",
    "              \"llite\": \"Lustre filesystem usage (per mount)\",\n",
    "              \"lnet\": \"Lustre network usage (lnet)\",\n",
    "              \"mdc\": \"Lustre network usage (mdc)\",\n",
    "              \"mic\": \"MIC scheduler account (per hardware thread)\",\n",
    "              \"osc\": \"Lustre filesystem usage (osc)\",\n",
    "              \"block\": \"Block device statistics (per device)\",\n",
    "              \"cpu\": \"Scheduler accounting (per CPU)\",\n",
    "              \"mem\": \"Memory usage (per socket)\",\n",
    "              \"net\": \"Network device usage (per device)\",\n",
    "              \"nfs\": \"NFS system usage\",\n",
    "              \"numa\": \"NUMA statistics (per socket)\",\n",
    "              \"proc\": \"Process specific data (MaxRSS, executable name etc.)\",\n",
    "              \"ps\": \"Process statistics\",\n",
    "              \"sysv_shm\": \"SysV shared memory segment usage\",\n",
    "              \"tmpfs\": \"Ram-backed filesystem usage (per mount)\",\n",
    "              \"vfs\": \"Dentry/file/inode cache usage\",\n",
    "              \"vm\": \"Virtual memory statistics\"\n",
    "                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops in loops in loops (Cleaning data)\n",
    "**Notes:**\n",
    "    - If a value is missing from the data, it will be replaced with '0' for the purpose of this project\n",
    "    - If a type of statistic was not collected on the job, that column is dropped from the DataFrame\n",
    "    - Two files are created during each iteration:\n",
    "         1) A .csv of the descriptive statistics for that host,job pair\n",
    "         2) A full .csv of the host,job data from the formatted DataFrame\n",
    "    - Naming convention: Files are labelled as '{host}_{jobid}' to support random lookup\n",
    "         * A job run on multiple host nodes is processed and saved with each individual host,job pair *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "\n",
    "for job in range( 1 ): # len(valid_jobs)\n",
    "\n",
    "    # general job values\n",
    "    jobid = valid_jobs[job]\n",
    "    start = pd.to_datetime(round(jobid.start_time), unit='s').time()\n",
    "    end = pd.to_datetime(round(jobid.end_time), unit='s').time()\n",
    "    numCycles = len(jobid.times) + 2\n",
    "    total += 1\n",
    "    type_avgs = {}\n",
    "    times = []\n",
    "    \n",
    "    # check that stat types all exist in master list\n",
    "    # append if not found\n",
    "    for stat in jobid.schemas.keys():\n",
    "        if stat not in stat_types:\n",
    "            stat_types.append(stat)\n",
    "    \n",
    "    # iterate through each host object job was run on\n",
    "    for host_name, host in jobid.hosts.iteritems():\n",
    "        typ='all'\n",
    "        dev='all'\n",
    "        \n",
    "        if 'all' in typ: typ_keys = host.stats.keys()\n",
    "        else: typ_keys = typ\n",
    "        \n",
    "        # convert timestamps to DateTime objects\n",
    "        times = [ pd.to_datetime(round(time), unit='s').time() for time in host.times ]\n",
    "        \n",
    "        # iterate through all types of stats\n",
    "        print(\"typ_keys:\", typ_keys)\n",
    "        for type_name in typ_keys:\n",
    "            type_data = host.stats[type_name]\n",
    "            type_avgs[type_name] = []\n",
    "            \n",
    "            \n",
    "            if 'all' in dev: dev_keys = type_data.keys()\n",
    "            else: dev_keys = dev\n",
    "            \n",
    "            # iterate through all devices each stat was collected on\n",
    "            # compute the mean of the row for each row in matrix\n",
    "            print(\"dev_keys:\", dev_keys)\n",
    "            for dev_name in dev_keys:          \n",
    "                type_avgs[type_name].append(\n",
    "                    [  np.mean(row).round(decimals=3) for row in type_data[dev_name] ])\n",
    "        \n",
    "        # skeleton for job DataFrame\n",
    "        all_stats = { time:{ stat:0 for stat in type_avgs } for time in times }\n",
    "        \n",
    "        # skeleton for descriptive stats DataFrame\n",
    "        dev_stats = { type_stat:{} for type_stat in jobid.schemas.keys() }\n",
    "        \n",
    "        # parse through all values in matrices\n",
    "        # assign row,col pair in matrix to row,col pair in df\n",
    "        print(\"type_avgs.iteritems():\", type_avgs.iteritems)\n",
    "        for type_avg,data in type_avgs.iteritems():\n",
    "            \n",
    "            dev_stats[type_avg][\"Overall Average\"] = np.mean(data)\n",
    "            dev_stats[type_avg][\"Standard Deviation\"] = np.std(data)\n",
    "            dev_stats[type_avg][\"High\"] = np.max(data)\n",
    "            dev_stats[type_avg][\"Low\"] = np.min(data)\n",
    "            \n",
    "            print(\"data:\", data)\n",
    "            for dev in range( len(data) ):\n",
    "                for i in range( len(times)):\n",
    "                    try:\n",
    "                        all_stats[times[i]][type_avg] = data[dev][i].round(3)\n",
    "                        \n",
    "                    except:\n",
    "                        next\n",
    "        \n",
    "        # clean up descriptive stats and save to csv\n",
    "        desc_df = (formatTitles( pd.DataFrame( dev_stats )).dropna(axis=1)).set_index('Unnamed: 0')\n",
    "        desc_df.to_csv(path_or_buf=\"./jobs/descriptive_stats/{host}_{jobid}.csv\"\n",
    "                       .format( jobid=valid_jobs[job].id, host=host_name ))\n",
    "        \n",
    "        # create df\n",
    "        df = pd.DataFrame(all_stats)\n",
    "        \n",
    "        # transpose df so index=time, col=category of stat\n",
    "        # relable columns to formal title\n",
    "        df = formatTitles( df.T )\n",
    "        df.index.name = \"Cycle\"\n",
    "        \n",
    "        # save jobs to csv\n",
    "        # set some aside for testing model\n",
    "        if total <= (len(valid_jobs)*3/4):\n",
    "            df.to_csv(path_or_buf=\"./jobs/train/{host}_{jobid}.csv\"\n",
    "                           .format( jobid=valid_jobs[job].id, host=host_name ))\n",
    "        else:\n",
    "            df.to_csv(path_or_buf=\"./jobs/test/{host}_{jobid}.csv\"\n",
    "                           .format( jobid=valid_jobs[job].id, host=host_name ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that no job was missed\n",
    "if total == len(valid_jobs):\n",
    "    print \"Success!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example read of descriptive .csv file saved\n",
    "ex_desc_df = pd.DataFrame( pd.read_csv(\"./jobs/descriptive_stats/{host_A}_{job_A}.csv\"\n",
    "                          .format( host_A=ex_job_host, job_A=ex_job)) )\n",
    "ex_desc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example read of full .csv file saved\n",
    "ex_df = pd.DataFrame( pd.read_csv(\"./jobs/train/{host_A}_{job_A}.csv\"\n",
    "                          .format( host_A=ex_job_host, job_A=ex_job)) )\n",
    "ex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stat_types)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
